{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project for Application of Differential Privacy with PATE\n",
    "<font size=\"3\">\n",
    "This project is to show the application of differential privacy (DP) with PATE (Private Aggregation of Teacher Ensembles) algorithm on MNIST handwritten digits dataset. You will get the intuition on the relationship between privacy loss and accuracy of differenial-privacy outputs. For details, please refer to README file. <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWYlf89SGZBy"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d39NqjSnGZB3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 60000\n",
      "Testset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define a transform \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Load train and test data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "print('Trainset size:', len(trainset))\n",
    "print('Testset size:', len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1565283166762,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "XXMUwQxBd19k",
    "outputId": "059f372f-3b5a-44a0-bd72-018b650cb3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "model = Net()\n",
    "print(model)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a device to use GPU \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training of Teacher Models and Prediction on Student Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7GddvNMGZCC"
   },
   "outputs": [],
   "source": [
    "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
    "    \n",
    "    # Make a new directory to contain prediction files for each teacher model              \n",
    "    dirName = \"teachers \" + str(num_teachers)\n",
    "    if not os.path.exists(dirName):\n",
    "        os.mkdir(dirName)\n",
    "        \n",
    "    # Change directory to save the files in the created directory\n",
    "    os.chdir(dirName) \n",
    "    \n",
    "    # Train teacher models\n",
    "    for i in range(num_teachers):\n",
    "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
    "        train = Subset(trainset, train_idx)\n",
    "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
    "       \n",
    "        model = Net()\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
    "        model.to(device)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            for images, labels in trainloader:                 \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()        \n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()        \n",
    "                running_loss += loss.item() \n",
    "                \n",
    "        # Print training loss for each teacher model     \n",
    "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
    "        \n",
    "        # Define student dataset \n",
    "        student_idx = list(range(0, 6000))      \n",
    "        student_data = Subset(testset, student_idx)  \n",
    "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
    "        \n",
    "        # With the trained teacher models, make predictions on student dataset       \n",
    "        model.to(device) \n",
    "        model.eval()\n",
    "        \n",
    "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
    "        for images, labels in student_loader:             \n",
    "            images, labels = images.to(device), labels.to(device)                             \n",
    "            output = model.forward(images)\n",
    "            pred = torch.argmax(torch.exp(output), dim=1)\n",
    "            outputs = torch.cat((outputs, pred))            \n",
    "    \n",
    "        # Save outputs as .pt file for use in case of kernel-restart, so that retraining is not needed.\n",
    "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
    "        torch.save(outputs, file_name)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher_1 training loss: 0.5744781086319372\n",
      "Teacher_2 training loss: 0.41659486293792725\n",
      "Teacher_3 training loss: 0.560007454533326\n",
      "Teacher_4 training loss: 0.5285757889873103\n",
      "Teacher_5 training loss: 0.568955637906727\n",
      "Teacher_6 training loss: 0.4504693346588235\n",
      "Teacher_7 training loss: 0.6150174360526236\n",
      "Teacher_8 training loss: 0.6178547834095202\n",
      "Teacher_9 training loss: 0.5097571485920956\n",
      "Teacher_10 training loss: 0.5873075943244132\n",
      "Teacher_11 training loss: 0.6334171561818374\n",
      "Teacher_12 training loss: 0.5250420617429834\n",
      "Teacher_13 training loss: 0.5332103591216238\n",
      "Teacher_14 training loss: 0.6178819599904513\n",
      "Teacher_15 training loss: 0.6038057584511606\n",
      "Teacher_16 training loss: 0.4882370766840483\n",
      "Teacher_17 training loss: 0.5581442792164651\n",
      "Teacher_18 training loss: 0.5499839453320754\n",
      "Teacher_19 training loss: 0.5141241487703825\n",
      "Teacher_20 training loss: 0.481162965297699\n",
      "Teacher_21 training loss: 0.596268609950417\n",
      "Teacher_22 training loss: 0.48046356439590454\n",
      "Teacher_23 training loss: 0.5866554050069106\n",
      "Teacher_24 training loss: 0.5329524545293105\n",
      "Teacher_25 training loss: 0.5237714460021571\n",
      "Teacher_26 training loss: 0.6254947875675402\n",
      "Teacher_27 training loss: 0.5931538343429565\n",
      "Teacher_28 training loss: 0.5775459800895891\n",
      "Teacher_29 training loss: 0.548269797312586\n",
      "Teacher_30 training loss: 0.5044717537729364\n",
      "Teacher_31 training loss: 0.5339929837929575\n",
      "Teacher_32 training loss: 0.6113480988301729\n",
      "Teacher_33 training loss: 0.605812962117948\n",
      "Teacher_34 training loss: 0.5392575781596335\n",
      "Teacher_35 training loss: 0.5480739646836331\n",
      "Teacher_36 training loss: 0.5915233132086302\n",
      "Teacher_37 training loss: 0.5646732747554779\n",
      "Teacher_38 training loss: 0.509417482896855\n",
      "Teacher_39 training loss: 0.5987867038500937\n",
      "Teacher_40 training loss: 0.611447955432691\n",
      "Teacher_41 training loss: 0.5062104824342226\n",
      "Teacher_42 training loss: 0.5240063055565483\n",
      "Teacher_43 training loss: 0.5707630373929676\n",
      "Teacher_44 training loss: 0.5543615723911085\n",
      "Teacher_45 training loss: 0.59913191356157\n",
      "Teacher_46 training loss: 0.5056818378599066\n",
      "Teacher_47 training loss: 0.501663874638708\n",
      "Teacher_48 training loss: 0.4985017447095168\n",
      "Teacher_49 training loss: 0.4496860927657077\n",
      "Teacher_50 training loss: 0.36166725111635106\n"
     ]
    }
   ],
   "source": [
    "# Execute the above function\n",
    "\n",
    "num_teachers = 50\n",
    "num_examples = len(trainset)//num_teachers\n",
    "epochs = 10\n",
    "\n",
    "get_teacher_preds(num_teachers, num_examples, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/USER/private-ai-master\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory to load prediction files\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case current working directory is not 'teachers 50', uncomment the command below and run the command\n",
    "# os.chdir(\"teachers 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/USER/private-ai-master/teachers 50\n"
     ]
    }
   ],
   "source": [
    "# Make sure that current working directory is 'teachers 50'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prediction files generated in the training process and merge loaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1565283682681,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "LNstgEpPGZCK",
    "outputId": "b3eab531-0505-4e7a-b5ea-140513a4561d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 6000)\n"
     ]
    }
   ],
   "source": [
    "num_teachers = 50\n",
    "\n",
    "preds = []\n",
    "for i in range(num_teachers): \n",
    "    file_name = 'pred_' + str(i+1) + '.pt'\n",
    "    pred = torch.load(file_name).cpu().numpy()\n",
    "    preds.append(pred)    \n",
    "    teacher_preds = np.vstack((preds))\n",
    "print(teacher_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DP- aggregated labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qMGJ9IXGZCq"
   },
   "outputs": [],
   "source": [
    "def aggragate_teacher_preds(epsilon):  \n",
    "    \n",
    "    labels = np.array([]).astype(int)\n",
    "    for pred in np.transpose(teacher_preds):   \n",
    "        label_counts = np.bincount(pred, minlength=10)    \n",
    "        beta = 1 / epsilon\n",
    "\n",
    "        for i in range(len(label_counts)):\n",
    "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "        \n",
    "        new_label = np.argmax(label_counts)   \n",
    "        labels = np.append(labels, new_label) \n",
    "        labels =  torch.from_numpy(labels)      \n",
    "   \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2029,
     "status": "ok",
     "timestamp": 1565283721570,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "ueSL8_v8GZC0",
    "outputId": "21616673-19e6-413f-e027-587853f024c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1565283725078,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "w_rhuXEzGZC4",
    "outputId": "f4fef1eb-d67b-43af-9085-1e6fcf08607e"
   },
   "outputs": [],
   "source": [
    "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
    "diff_priv_labels = aggragate_teacher_preds(0.2)  # true answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5159,
     "status": "ok",
     "timestamp": 1565283732726,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "yTTQcDxlGZC8",
    "outputId": "9972f256-df4c-4caa-9f1a-7d0e305b7966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 971.5129254649704\n",
      "Data Dependent Epsilon: 106.92126000063695\n"
     ]
    }
   ],
   "source": [
    "# Perform PATE Analysis\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=teacher_preds, indices=diff_priv_labels, noise_eps=0.2, delta=1e-5)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a new model on labeled student dataset . This will be the final \"DP\" model.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46483,
     "status": "ok",
     "timestamp": 1565283787702,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "86NISKafGZDA",
    "outputId": "ccf86270-e7ff-46ab-b24c-db6fb6894bc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15    Training Loss: 1.54432   \n",
      "Epoch: 2/15    Training Loss: 0.58701   \n",
      "Epoch: 3/15    Training Loss: 0.44037   \n",
      "Epoch: 4/15    Training Loss: 0.38858   \n",
      "Epoch: 5/15    Training Loss: 0.34863   \n",
      "Epoch: 6/15    Training Loss: 0.31705   \n",
      "Epoch: 7/15    Training Loss: 0.30802   \n",
      "Epoch: 8/15    Training Loss: 0.28909   \n",
      "Epoch: 9/15    Training Loss: 0.27849   \n",
      "Epoch: 10/15    Training Loss: 0.26019   \n",
      "Epoch: 11/15    Training Loss: 0.26151   \n",
      "Epoch: 12/15    Training Loss: 0.25292   \n",
      "Epoch: 13/15    Training Loss: 0.26207   \n",
      "Epoch: 14/15    Training Loss: 0.24103   \n",
      "Epoch: 15/15    Training Loss: 0.23573   \n"
     ]
    }
   ],
   "source": [
    "# Redefine student train data with aggragated labels  \n",
    "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
    "testset.targets[:6000] = diff_priv_labels\n",
    "\n",
    "student_train = Subset(testset, list(range(6000)))\n",
    "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
    "\n",
    "model_2 = Net()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "model_2.to(device)\n",
    "epochs = 15\n",
    "\n",
    "for e in range(epochs): \n",
    "    train_loss = 0.0    \n",
    "    for images, labels in student_trainloader:          \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "        output = model_2(images)        \n",
    "        loss = criterion(output, labels)        \n",
    "        loss.backward()       \n",
    "        optimizer.step()        \n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  With the student model, make predictions on student test-data and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1565283791446,
     "user": {
      "displayName": "Jung Soon Noh",
      "photoUrl": "",
      "userId": "09385285381827753300"
     },
     "user_tz": -540
    },
    "id": "5PmNoVQEGZDN",
    "outputId": "bc7b9b88-4ed3-4e75-872a-077d2698ad75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.17437    Test Accuracy: 0.94940\n"
     ]
    }
   ],
   "source": [
    "student_test = Subset(testset, list(range(6000, 10000)))  \n",
    "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
    "\n",
    "test_loss = 0\n",
    "accuracy = 0\n",
    "model_2.to(device)\n",
    "model_2.eval()\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for images, labels in student_testloader:              \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model_2(images)\n",
    "        test_loss += criterion(output, labels).item()        \n",
    "               \n",
    "        # Calculate test accuracy\n",
    "        ps = torch.exp(output)\n",
    "        top_p, top_class = ps.topk(1, dim=1)        \n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
    "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DP - PATE-Copy1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
