{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Credit Default Risk Shared Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in Brazil that companies buy data from a bureau and then combine it with internal credit default indicators to create a risk model that drives concession. The credit default data is always siloed inside companies and since it is sensitive data, cannot be shared. But in the other hand, controlling default rate isn't usually the core business of these companies. \n",
    "\n",
    "So if there was a way for these companies to work together and achieve better default rates, while preserving its clients privacy... Enter Federated Learning and Encrypted Learning: this project aims to create a better credit default risk model by sharing a model between two or more companies, while still preserving customers' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch as th\n",
    "import syft as sy\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Companies**\n",
    "- Shiny: This is a fictitional company willing to rate its customers regarding their credit default risk.\n",
    "\n",
    "- High: Another fictitional company, also interested in rating its customers.\n",
    "\n",
    "**Bureau**\n",
    "- BestView - This is us, a fictitional bureau of data, a company that sells data and models, and which will be the the central point of the shared model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating virtual workers representing each party of our scenario\n",
    "shiny = sy.VirtualWorker(hook, id='shiny')\n",
    "high = sy.VirtualWorker(hook, id='high')\n",
    "best_view = sy.VirtualWorker(hook, id=\"best_view\")\n",
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the willing scenario, we'll use [Home Credit Default Risk Kaggle competition](https://www.kaggle.com/c/home-credit-default-risk/overview) data. \n",
    "\n",
    "In this demonstration, we're only interested in training the model, therefore, all testing steps are being ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity , only a few features will be used, and they're are all continuous.\n",
    "\n",
    "cols = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', \n",
    "        'DAYS_REGISTRATION', 'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "        'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG',\n",
    "        'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG','LIVINGAPARTMENTS_AVG',\n",
    "        'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', \n",
    "        'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', \n",
    "        'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', \n",
    "        'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n",
    "        'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI',\n",
    "        'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', \n",
    "        'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', \n",
    "        'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "        'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', \n",
    "        'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', \n",
    "        'AMT_REQ_CREDIT_BUREAU_YEAR', 'TARGET']\n",
    "\n",
    "data = pd.read_csv('application_train.csv', usecols=cols)\n",
    "target = data.pop('TARGET').to_numpy()\n",
    "\n",
    "# shuffling and spliting data into two parts, one for each fictitional company\n",
    "idx = data.index.to_numpy()\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "shiny_idx = idx[:10000]\n",
    "high_idx = idx[10000:20000]\n",
    "\n",
    "# filling nulls\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local differential privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that one of the companies is not yet tottaly comfortable with all this \"shared model\" idea, and wishes to add an extra level of privacy protection to its customers.\n",
    "\n",
    "One way it could be done is to add a \"plausible deniability\" to the data: in case of any data leakage one customer could argue that his data does not correspond to the truth, and in fact, that it has been set tottally at random. We'll do this by adding Local differential privacy:\n",
    "\n",
    "First we randomly choose a small percentage of the customers to flip a coin, and then, this coin flip is going to be responsible for the target variable of this customer (which means he has a credit default or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_percentage = (np.random.rand(len(target)) > 0.01).astype(int)\n",
    "coin_flip = (np.random.rand(len(target)) > 0.5).astype(int)\n",
    "\n",
    "dp_target = (target * small_percentage + (1-small_percentage) * coin_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we create the tensors and send each dataset to the corresponding company\n",
    "shiny_X = th.from_numpy(data[shiny_idx])\n",
    "shiny_y = th.from_numpy(dp_target[shiny_idx])\n",
    "shiny_dataset = sy.BaseDataset(shiny_X, shiny_y).send(shiny)\n",
    "\n",
    "high_X = th.from_numpy(data[high_idx])\n",
    "high_y = th.from_numpy(dp_target[high_idx])\n",
    "high_dataset = sy.BaseDataset(high_X, high_y).send(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining both datasets into a FederatedDataset\n",
    "federated_train_dataset = sy.FederatedDataset([high_dataset, shiny_dataset])\n",
    "\n",
    "# and then, creating our DataLoader\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, \n",
    "                                                shuffle=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define a model\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(65, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and also our criterion and optmizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we'll iterate through the epochs and batches, and we'll keep sending the model to each company (worker) - since the data is located there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 41.7953967154026\n",
      "Epoch 2 - Loss: 30.085354417562485\n",
      "Epoch 3 - Loss: 26.08218541741371\n",
      "Epoch 4 - Loss: 24.370345383882523\n",
      "Epoch 5 - Loss: 23.718352928757668\n",
      "Epoch 6 - Loss: 23.366930544376373\n",
      "Epoch 7 - Loss: 23.17524318397045\n",
      "Epoch 8 - Loss: 23.099843487143517\n",
      "Epoch 9 - Loss: 22.716980166733265\n",
      "Epoch 10 - Loss: 22.870363876223564\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        worker = inputs.location\n",
    "        model.send(worker)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        model.get()\n",
    "        epoch_loss += loss.get().item()\n",
    "        \n",
    "    print('Epoch {} - Loss: {}'.format(e+1,epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's say we want other companies to be able to score their customers too (selling it as Machine Learning as a Service), but we don't want to expose our gradients. On the other hand, these companies don't want us to have access to their data either. \n",
    "\n",
    "One way to do that, is to encrypt our model AND the data, share all of it and do the whole scoring process while encrypted. We'll demonstrate it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's simulate this new company and its data\n",
    "new_company = sy.VirtualWorker(hook, id='new_company')\n",
    "new_company_idx = idx[20000:30000]\n",
    "\n",
    "# and also, let's share the data between the Bureau and the company\n",
    "new_company_X = th.from_numpy(data[new_company_idx]).fix_precision().share(new_company, best_view, crypto_provider=secure_worker, requires_grad=True)\n",
    "new_company_y = th.from_numpy(dp_target[new_company_idx]).fix_precision().share(new_company, best_view, crypto_provider=secure_worker, requires_grad=True)\n",
    "\n",
    "# we'll also share the already trained model between the same parties\n",
    "model = model.fix_precision().share(new_company, best_view, crypto_provider=secure_worker, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we can get an output using an encrypted model and encrypted data\n",
    "output = model(new_company_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's also easy to retrieve the predictions from the encrypted output\n",
    "predictions = output.get().float_precision()\n",
    "predictions = F.softmax(predictions, dim=0).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We have: \n",
    "- Added Local Differential Privacy\n",
    "- Trained a model using Federated Learning\n",
    "- Encrypted a model and forwarded it through encrypted data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
