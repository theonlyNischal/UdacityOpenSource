{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted classification (with Bob and Alice)\n",
    "## This project uses Pytorch and Pysyft to perform encrypted classification, distributed on two workers, on images from the MNIST database.\n",
    "A simple (so that can be easily trained on CPU) fully connected network with one hidden layer is built using Pytorch, and trained on the MNIST database. Both the data and the trained model are then encrypted using a private additive sharing encryption scheme, and sent to our two workers Bob and Alice for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 12:56:44.243318  3484 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\apopesc2\\AppData\\Local\\Continuum\\anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0812 12:56:44.291685  3484 deprecation_wrapper.py:119] From C:\\Users\\apopesc2\\AppData\\Local\\Continuum\\anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted\\session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hook our workers and define the `crypto_provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch) \n",
    "bob = syft.VirtualWorker(hook, id=\"bob\")\n",
    "alice = syft.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = syft.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class of hyperparameters and instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 100\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.001\n",
    "\n",
    "args = Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download, process MNIST data, and build the training and testing dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model: fully conected network with one hidden layer and Relu activation, going from the input size of 28x28 to the hidden layer size 512 and to the output size of 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch: {} -> {:.0f}% done...'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader)))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 -> 0% done...\n",
      "Epoch: 1 -> 11% done...\n",
      "Epoch: 1 -> 21% done...\n",
      "Epoch: 1 -> 32% done...\n",
      "Epoch: 1 -> 43% done...\n",
      "Epoch: 1 -> 53% done...\n",
      "Epoch: 1 -> 64% done...\n",
      "Epoch: 1 -> 75% done...\n",
      "Epoch: 1 -> 85% done...\n",
      "Epoch: 1 -> 96% done...\n",
      "Epoch: 2 -> 0% done...\n",
      "Epoch: 2 -> 11% done...\n",
      "Epoch: 2 -> 21% done...\n",
      "Epoch: 2 -> 32% done...\n",
      "Epoch: 2 -> 43% done...\n",
      "Epoch: 2 -> 53% done...\n",
      "Epoch: 2 -> 64% done...\n",
      "Epoch: 2 -> 75% done...\n",
      "Epoch: 2 -> 85% done...\n",
      "Epoch: 2 -> 96% done...\n",
      "Epoch: 3 -> 0% done...\n",
      "Epoch: 3 -> 11% done...\n",
      "Epoch: 3 -> 21% done...\n",
      "Epoch: 3 -> 32% done...\n",
      "Epoch: 3 -> 43% done...\n",
      "Epoch: 3 -> 53% done...\n",
      "Epoch: 3 -> 64% done...\n",
      "Epoch: 3 -> 75% done...\n",
      "Epoch: 3 -> 85% done...\n",
      "Epoch: 3 -> 96% done...\n",
      "Epoch: 4 -> 0% done...\n",
      "Epoch: 4 -> 11% done...\n",
      "Epoch: 4 -> 21% done...\n",
      "Epoch: 4 -> 32% done...\n",
      "Epoch: 4 -> 43% done...\n",
      "Epoch: 4 -> 53% done...\n",
      "Epoch: 4 -> 64% done...\n",
      "Epoch: 4 -> 75% done...\n",
      "Epoch: 4 -> 85% done...\n",
      "Epoch: 4 -> 96% done...\n",
      "Epoch: 5 -> 0% done...\n",
      "Epoch: 5 -> 11% done...\n",
      "Epoch: 5 -> 21% done...\n",
      "Epoch: 5 -> 32% done...\n",
      "Epoch: 5 -> 43% done...\n",
      "Epoch: 5 -> 53% done...\n",
      "Epoch: 5 -> 64% done...\n",
      "Epoch: 5 -> 75% done...\n",
      "Epoch: 5 -> 85% done...\n",
      "Epoch: 5 -> 96% done...\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the encrypted model and data to Bob and Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "        data.fix_prec().share(alice, bob, crypto_provider=crypto_provider),\n",
    "        target.fix_prec().share(alice, bob, crypto_provider=crypto_provider)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct_priv = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1) \n",
    "            n_total += args.test_batch_size\n",
    "            \n",
    "            # get and decrypt data from workers (inspired from Theo Ryffel's Openmined post)\n",
    "            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n",
    "            n_correct = n_correct_priv.copy().get().float_precision().long().item()\n",
    "    \n",
    "            print('Test accuracy: {:.0f}%'.format(100. * n_correct / n_total))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97%\n",
      "Test accuracy: 97%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 97%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n",
      "Test accuracy: 98%\n"
     ]
    }
   ],
   "source": [
    "test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project shows that it is posible to obtaine a good accuracy by performing encrypted inference on privately held data with an encrypted trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
