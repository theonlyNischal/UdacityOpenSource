{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-rnn-classification",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirupamait/federated-learning-on-raspberry-pi/blob/master/Char_rnn_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiVRHoWisYNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import syft as sy\n",
        "import pandas as pd\n",
        "import random\n",
        "from syft.frameworks.torch.federated import utils\n",
        "\n",
        "from syft.workers import WebsocketClientWorker\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v5vV97xw2v2",
        "colab_type": "code",
        "outputId": "502f72da-adf1-40f3-f260-9ed9dadf9669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "!wget https://download.pytorch.org/tutorial/data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-18 18:24:58--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.224.29.60, 13.224.29.19, 13.224.29.73, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.224.29.60|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-08-18 18:24:58 (53.2 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nw3Q1eBxT-w",
        "colab_type": "code",
        "outputId": "0ee9f0c7-48f1-4cd9-de49-16139ab17a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2BPWFLQxYn8",
        "colab_type": "code",
        "outputId": "f4ade389-4d1b-4722-e3dc-0fcad540daa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "all_filenames = glob.glob('data/names/*.txt')\n",
        "print(all_filenames)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data/names/Italian.txt', 'data/names/Irish.txt', 'data/names/Dutch.txt', 'data/names/Scottish.txt', 'data/names/Vietnamese.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt', 'data/names/French.txt', 'data/names/Chinese.txt', 'data/names/Russian.txt', 'data/names/Polish.txt', 'data/names/German.txt', 'data/names/Korean.txt', 'data/names/Greek.txt', 'data/names/English.txt', 'data/names/Arabic.txt', 'data/names/Czech.txt', 'data/names/Spanish.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWEWJuAP-tRg",
        "colab_type": "text"
      },
      "source": [
        "**Data Preparation Steps::**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z5Gu9pb-275",
        "colab_type": "text"
      },
      "source": [
        "1.)**Conversion from unicode to ascii**\n",
        "     Why to convert from unicode to ascii:: Char-rnn, supports ascii and will error out in nonmasking      characters(unicode with type as diacritic.)\n",
        "     \n",
        " **Code to convert from unicode to ascii  is** def unicodeToAscii(s): ....\n",
        "    We have to remove only diacritical marks .We need to normalize the unicode string using NFKD-normalization and then remove only those unicode whose category type is Mn.\n",
        "Mn ::Nonspacing character that indicates modifications of a base character. Signified by the Unicode designation \"Mn\" (mark, nonspacing). The value is 5.\n",
        "Unicode normalization:: unicodedata.normalize(form, unistr)::Return the normal form form for the Unicode string unistr. Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.The normalization does, it makes sure characters which look identical actually are identical.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kx6QyG1_5Fm",
        "colab_type": "text"
      },
      "source": [
        "**2)Build category_lines dictionary, a list of names per language**::\n",
        "eg:: category_lines will be like {'Japanese': ['Abe', 'Abukara'...],....'Irish':['Adam', 'Ahearn', 'Aodh'])\n",
        "\n",
        "**Build all_categories list for list of all 18 Countries. eg::**\n",
        "all_categoroes will like ['Japanese', 'Irish', 'Italian', 'Dutch', 'Russian', 'German',...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C77E2nk_A6Su",
        "colab_type": "text"
      },
      "source": [
        "**3)Convert the word in each category into one-hot vector in order to feed into RNN neural network model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5m3zB8wnrN9",
        "colab_type": "code",
        "outputId": "bbaac151-5638-4c13-8005-bbe82f6f68f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "print(findFiles('data/names/*.txt'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data/names/Italian.txt', 'data/names/Irish.txt', 'data/names/Dutch.txt', 'data/names/Scottish.txt', 'data/names/Vietnamese.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt', 'data/names/French.txt', 'data/names/Chinese.txt', 'data/names/Russian.txt', 'data/names/Polish.txt', 'data/names/German.txt', 'data/names/Korean.txt', 'data/names/Greek.txt', 'data/names/English.txt', 'data/names/Arabic.txt', 'data/names/Czech.txt', 'data/names/Spanish.txt']\n",
            "Slusarski\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0yqB6P4H_dN",
        "colab_type": "code",
        "outputId": "9322be5f-45ed-4fa0-c502-545154bc8de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('J'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}