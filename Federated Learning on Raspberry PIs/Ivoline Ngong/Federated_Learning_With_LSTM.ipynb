{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning With LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyclare/federated-learning-on-raspberry-pi/blob/ivy_branch/Federated_Learning_With_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-xSOW4tnAB",
        "colab_type": "text"
      },
      "source": [
        "# The Complete Beginners Guide to Federated Learning With LSTM \n",
        "on Movie Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_PVIqRct34M",
        "colab_type": "text"
      },
      "source": [
        "We are going to implement this in the following steps:\n",
        "- Create  devices (Virtual Workers)\n",
        "- Distribute our data to those devices\n",
        "- Create our model\n",
        "- Send our model to the devices (Cause our model is located in our computer , while the data is located in their machines)\n",
        "- Do normal Training\n",
        "-Get the smarter model back from devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMrO0VTiKED1",
        "colab_type": "code",
        "outputId": "0205fb8b-4c8d-4403-86f4-40d9a46ceacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Change To The WORKING DIRECTORY\n",
        "%cd /content/drive/My Drive/Colab Notebooks/PrivateAI\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/PrivateAI\n",
            "/content/drive/My Drive/Colab Notebooks/PrivateAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrml_6gXpvNq",
        "colab_type": "text"
      },
      "source": [
        "## Installing Pysyft\n",
        "\n",
        "Pysyft is an extension of Pytorch that is needed inorder to perform Federated Learning. Since we are using Google Colab for this project, we only need to run the command below to install Pysyft and we are good to go. \n",
        "\n",
        "If you are not using Google Colab, please follow the instructions here , to set up your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDv0Tw0ntSZI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2If3NDoptXl",
        "colab_type": "code",
        "outputId": "49088d2d-0d85-4b1c-9201-fe3732867019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/25/633ddb891b3c4927bd03311a04ece038387faecb46120b8429ed28c72c13/syft-0.1.23a1-py3-none-any.whl (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Collecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 26.0MB/s \n",
            "\u001b[?25hCollecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Collecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting tf-encrypted!=0.5.7,>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/82/cf15aeac92525da2f794956712e7ebf418819390dec783430ee242b52d0b/tf_encrypted-0.5.8-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.2MB/s \n",
            "\u001b[?25hCollecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 30.7MB/s \n",
            "\u001b[?25hCollecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Collecting python-socketio>=4.3.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/b0/22c3f785f23fec5c7a815f47c55d7e7946a67ae2129ff604148e939d3bdb/python_socketio-4.3.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted!=0.5.7,>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Collecting python-engineio>=3.9.0 (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/20/8e3ba16102ae2e245d70d9cb9fa48b076253fdb036dc43eea142294c2897/python_engineio-3.9.3-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.1.0-cp36-cp36m-linux_x86_64.whl size=1067106 sha256=a3fca792cca767034ab0b2581939af5b0abacf8cf3b6792c628c5ee377bf318f\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44105 sha256=250195edbf14e1ed69e9c1ed2681e8cec3dbe92c8608c1dc0b032afc06dfa029\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: msgpack, lz4, websocket-client, python-engineio, python-socketio, flask-socketio, pyyaml, tf-encrypted, websockets, zstd, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.2.1 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.9.3 python-socketio-4.3.1 pyyaml-5.1.2 syft-0.1.23a1 tf-encrypted-0.5.8 websocket-client-0.56.0 websockets-8.0.2 zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5a4RIWvqiuu",
        "colab_type": "text"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYvcu_S8vywd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The next step is to import the required libraries and hook Pytorch using  *sy.TorchHook* which makes the extended extended functions on Pytorch tensors available to us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-nM6q20qokW",
        "colab_type": "code",
        "outputId": "d50ea44b-4191-474d-be59-4db7110d5162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 05:16:27.754048 140679342729088 hook.py:98] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IttXs13SqXMJ",
        "colab_type": "text"
      },
      "source": [
        "## Creating New Workers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcbrKu2Xx6lQ",
        "colab_type": "text"
      },
      "source": [
        "As earlier mentioned, in order to perform Federated Learning we need to have data on more different devices and we have to send our model to those devices where the training will be done. Hence, we need to create 2 devices. We will assume the person with the first device is called Bob and the second device called Alice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY4fDBDKqDSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSYc5nG-9Rec",
        "colab_type": "text"
      },
      "source": [
        "## The LSTM Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fi0W2da9Xw8",
        "colab_type": "text"
      },
      "source": [
        "A short description of lstm and how they work  here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaOhDrxAs6Os",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries for LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw7AdIiCq3q-",
        "colab_type": "code",
        "outputId": "01941abe-05aa-4423-f8de-4fd9084fa60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    device = torch.device('cuda')\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    \n",
        "device"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is not available.  Training on CPU ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFdY_VTy1JXD",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quzpwmNl2tC2",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the IMDB dataset which is provided in Pytorch in the [torchtext.data.Dataset](https://torchtext.readthedocs.io/en/latest/datasets.html#imdb). \n",
        "And we split the data into train and test sets. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR1v_7XY1Ii1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ6TOOch4nod",
        "colab_type": "text"
      },
      "source": [
        "We take a look at what our data and then split the training data into a train set and validation set. 80% of the data is used for training and 10% for validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWKMf7434a2",
        "colab_type": "code",
        "outputId": "e98a0e51-ba6d-4bde-b1fe-a9f2f190418b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(reviews[:5000])\n",
        "print()\n",
        "print(labels[:20])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home  the entertainment sets  a bathroom  pictures on the wall  a computer  and everything you once treasure to see what it  s like to be homeless  that is goddard bolt  s lesson .  br    br   mel brooks  who directs  who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival  jeffery tambor  to see if he can live in the streets for thirty days without the luxuries if bolt succeeds  he can do what he wants with a future project of making more buildings . the bet  s on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can  t step off the sidewalk . he  s given the nickname pepto by a vagrant after it  s written on his forehead where bolt meets other characters including a woman by the name of molly  lesley ann warren  an ex  dancer who got divorce before losing her home  and her pals sailor  howard morris  and fumes  teddy wilson  who are already used to the streets . they  re survivors . bolt isn  t . he  s not used to reaching mutual agreements like he once did when being rich where it  s fight or flight  kill or be killed .  br    br   while the love connection between molly and bolt wasn  t necessary to plot  i found  life stinks  to be one of mel brooks  observant films where prior to being a comedy  it shows a tender side compared to his slapstick work such as blazing saddles  young frankenstein  or spaceballs for the matter  to show what it  s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don  t know what to do with their money . maybe they should give it to the homeless instead of using it like monopoly money .  br    br   or maybe this film will inspire you to help others .  \n",
            "airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belonging to rich businessman philip stevens  james stewart  who is flying them  a bunch of vip  s to his estate in preparation of it being opened to the public as a museum  also on board is stevens daughter julie  kathleen quinlan   her son . the luxury jetliner takes off as planned but mid  air the plane is hi  jacked by the co  pilot chambers  robert foxworth   his two accomplice  s banker  monte markham   wilson  michael pataki  who knock the passengers  crew out with sleeping gas  they plan to steal the valuable cargo  land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean  loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle . with air in short supply  water leaking in  having flown over    miles off course the problems mount for the survivor  s as they await help with time fast running out . . .  br    br   also known under the sligh\n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnueSWvALp6t",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD1uC5Rf5i-y",
        "colab_type": "code",
        "outputId": "af3e8f65-7f4a-4577-f77f-b3c2caa883b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_9AIswL2J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split by new lines and spaces\n",
        "\n",
        "reviews_split = all_text.split('\\n')\n",
        "\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wwo2OXrL-Ab",
        "colab_type": "code",
        "outputId": "282341be-3961-4ab1-ea95-2c5f9cae7b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldG1B0KAMAtw",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqkMO1UTMDdl",
        "colab_type": "code",
        "outputId": "32b5c4c7-99ac-40fa-a3f1-d3cde38ed58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts =  Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "  \n",
        "\n",
        "  # stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMKcAj32NsKW",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdUeZcT1Nfxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_lKiW_OOd4",
        "colab_type": "text"
      },
      "source": [
        "### Removing outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6d9Mrb_N5d3",
        "colab_type": "code",
        "outputId": "f8e65d9c-9b76-418d-e567-3029fec7fef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiWJdI2DO_rz",
        "colab_type": "code",
        "outputId": "755a19eb-885e-4a81-db01-c8b8c380470b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7vJBo63PDkn",
        "colab_type": "text"
      },
      "source": [
        "### Padding Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMp9rRrcPMhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''    \n",
        "    #getting the correct row x col \n",
        "    features= np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    \n",
        "    # for each review, grab that review and \n",
        "    for i, row in enumerate(reviews_ints):\n",
        "      features[i, -len(row):] =  np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJwgp-DPOPR",
        "colab_type": "code",
        "outputId": "d69c3416-23a7-4e73-ce09-8eed13f0d8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEDwMCuhPaN8",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbL70XcbPZbK",
        "colab_type": "code",
        "outputId": "308f2bc4-5dea-4672-dc10-1813a3b45880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\\n\".format(train_x.shape),\n",
        "      \"Val set: \\t\\t{}\\n\".format(val_x.shape),\n",
        "      \"Test set: \\t\\t{}\\n\".format(test_x.shape))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200)\n",
            " Val set: \t\t(2500, 200)\n",
            " Test set: \t\t(2500, 200)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnhtpW3Gzg55",
        "colab_type": "text"
      },
      "source": [
        "## Distributing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB5dUNfyznFC",
        "colab_type": "text"
      },
      "source": [
        "Our virtual workers have been created but they don't have any data on them. After loading our data, we distribute the data between Alice and Bob. We do this by splitting the training and validation datasets into 2 and sending them to the workers using the sy.BaseDataset class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxX9iFgsQ2nv",
        "colab_type": "code",
        "outputId": "2b9fdda1-f46f-48e7-8350-27ee91e6729f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_x.shape[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abQTzFkiP56a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_idx = int(train_x.shape[0]/2)\n",
        "valid_idx = int(val_x.shape[0]/2)\n",
        "test_idx = int(test_x.shape[0]/2)\n",
        "\n",
        "# Sending toy datasets to virtual workers\n",
        "bob_train_dataset = sy.BaseDataset(torch.from_numpy(train_x[:train_idx]), \n",
        "                                  torch.from_numpy(train_y[:train_idx])).send(bob)\n",
        "\n",
        "alice_train_dataset = sy.BaseDataset(torch.from_numpy(train_x[train_idx:]), \n",
        "                                    torch.from_numpy(train_y[train_idx:])).send(alice)\n",
        "\n",
        "\n",
        "bob_valid_dataset = sy.BaseDataset(torch.from_numpy(val_x[:valid_idx]), \n",
        "                                  torch.from_numpy(val_y[:valid_idx])).send(bob)\n",
        "                                     \n",
        "alice_valid_dataset = sy.BaseDataset(torch.from_numpy(val_x[valid_idx:]), \n",
        "                                  torch.from_numpy(val_y[valid_idx:])).send(alice)\n",
        "\n",
        "\n",
        "bob_test_dataset = sy.BaseDataset(torch.from_numpy(test_x[:test_idx]), \n",
        "                                  torch.from_numpy(test_y[:test_idx])).send(bob)\n",
        "alice_test_dataset = sy.BaseDataset(torch.from_numpy(test_x[test_idx:]), \n",
        "                                  torch.from_numpy(test_y[test_idx:])).send(alice)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrYOzb4VAdp",
        "colab_type": "text"
      },
      "source": [
        "## Creating Federated DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWIvhUP-SfpB",
        "colab_type": "text"
      },
      "source": [
        "Now, we load datasets using dataloaders. In Federated learning, we load datasets from different devices in a federated manner using **Federated DataLoaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ycJ0p6bSf6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating federated datasets, an extension of Pytorch TensorDataset class\n",
        "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, alice_train_dataset])\n",
        "federated_valid_dataset = sy.FederatedDataset([bob_valid_dataset, alice_valid_dataset])\n",
        "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, alice_test_dataset])\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# Creating federated dataloaders, an extension of Pytorch DataLoader class\n",
        "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, \n",
        "                                                shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "federated_valid_loader = sy.FederatedDataLoader(federated_valid_dataset, \n",
        "                                                shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, \n",
        "                                               shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sf9cJuhVYs9",
        "colab_type": "text"
      },
      "source": [
        "### Building Our Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXB0E8JaAh0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # define all layers\n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim,hidden_dim, n_layers, dropout=drop_prob, batch_first = True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        #fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        # Batch_size used for shaping data\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  \n",
        "        \n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid funtion\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCMy2s_jGA_y",
        "colab_type": "code",
        "outputId": "19b135e0-99b2-4f2c-eb54-5adf6828a7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2 \n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QadI4z0-VzC7",
        "colab_type": "text"
      },
      "source": [
        "## Training The Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLy9QgHVVyY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mdevtghdl9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': federated_train_loader, \n",
        "                    'val': federated_valid_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNi4h9mdz4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "    since = time.time()\n",
        "\n",
        "    history = dict()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    skip_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs,labels in dataloaders[phase]:\n",
        "                #inputs, labels = data.text, data.label\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # Location of current batch\n",
        "                worker = inputs.location  # <---- Where will send the model to\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # we need to clear out the hidden state of the LSTM,\n",
        "                        # detaching it from its history on the last instance.\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        model.hidden = model.hidden.send(worker)   # <---- These steps are crucial\n",
        "                        model.send(worker)   # <---- for Federated Learning\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    else:\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        model.hidden = model.hidden.send(worker)   # <---- These steps are crucial\n",
        "                        model.send(worker)   # <---- for Federated Learning\n",
        "                        \n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        \n",
        "                    # Get the model back to the local worker\n",
        "                    model.get() # <--- We need get the model back before sending to another worker\n",
        "\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                outputs = torch.round(torch.sigmoid(outputs))\n",
        "                corrects = (outputs == labels).float()\n",
        "                acc = corrects.sum()/len(corrects)\n",
        "                running_corrects += acc.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase+'_acc' in history:\n",
        "                # append the new number to the existing array at this slot \n",
        "                                   history[phase+'_acc'].append(epoch_acc)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "            if phase+'_loss' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_loss'].append(epoch_loss)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgQsjODyeAKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL3K3NXOWIt0",
        "colab_type": "code",
        "outputId": "57fcf0a7-0e3a-4b8a-e2da-d8556598dc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# net.train()\n",
        "# # train for some number of epochs\n",
        "# for e in range(epochs):\n",
        "#     # initialize hidden state\n",
        "#     #h = net.init_hidden(batch_size)\n",
        "#     losses = []\n",
        "\n",
        "#     # batch loop\n",
        "#     for inputs, labels in federated_train_loader:\n",
        "#         counter += 1\n",
        "        \n",
        "#         # Location of current batch\n",
        "#         worker = inputs.location  # <---- Where will send the model to\n",
        "\n",
        "#         if(train_on_gpu):\n",
        "#             inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "#         # Creating new variables for the hidden state, otherwise\n",
        "#         # we'd backprop through the entire training history\n",
        "#         h = tuple([each.data for each in h])        \n",
        "        \n",
        "        \n",
        "#         h = h.send(worker)   # <---- These steps are crucial\n",
        "#         net.send(worker)   # <---- for Federated Learning\n",
        "\n",
        "#         # zero accumulated gradients\n",
        "#         #net.zero_grad()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # get the output from the model\n",
        "#         output, h = net(inputs, h)\n",
        "\n",
        "#         # calculate the loss and perform backprop\n",
        "#         loss = criterion(output.squeeze(), labels.float())\n",
        "#         loss.backward()\n",
        "#         # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "#         nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         # Get the model back to the local worker\n",
        "#         net.get() # <--- We need get the model back before sending to another worker\n",
        "#         losses.append(loss.get())\n",
        "\n",
        "#         # loss stats\n",
        "#         if counter % print_every == 0:\n",
        "#             # Get validation loss\n",
        "#             val_h = net.init_hidden(batch_size)\n",
        "#             val_losses = []\n",
        "#             net.eval()\n",
        "#             for inputs, labels in valid_loader:\n",
        "#               # get current location\n",
        "#               worker = inputs.location             \n",
        "              \n",
        "#               # Creating new variables for the hidden state, otherwise\n",
        "#               # we'd backprop through the entire training history\n",
        "#               val_h = torch.Tensor(tuple([each.data for each in val_h])).send(worker)\n",
        "              \n",
        "              \n",
        "#               if(train_on_gpu):\n",
        "#                 inputs, labels = inputs.cuda(), labels.cuda()\n",
        "              \n",
        "#               # Send model to worker\n",
        "#               net.send(worker)\n",
        "\n",
        "#               output, val_h = net(inputs, val_h)\n",
        "#               val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "#               val_losses.append(val_loss.item())\n",
        "              \n",
        "#               net.get()\n",
        "\n",
        "#             net.train()\n",
        "#             print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "#                   \"Step: {}...\".format(counter),\n",
        "#                   \"Loss: {:.6f}...\".format(loss.item()),\n",
        "#                   \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f81c7b7d64ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <---- These steps are crucial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <---- for Federated Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'send'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vZ1OL67dpC",
        "colab_type": "text"
      },
      "source": [
        "### Creating Glove Embeddings \n",
        "Now, we have to create an embedding layer. We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. We use Glove create our embedding layer. \n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. To read more about Glove and how it can be used refer to this [blog post](https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAm0mQOG-Tgy",
        "colab_type": "code",
        "outputId": "62d86cfd-2870-42bb-ca09-e760b35049a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# build the vocabulary\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:43, 19.8MB/s]                           \n",
            "100%|█████████▉| 399133/400000 [00:28<00:00, 14058.08it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZc2OOwY-pjq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMkilHFP-n-M",
        "colab_type": "code",
        "outputId": "a2ab0b5d-4c97-4d48-8cc4-96d09dd9e3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQqYSWo_zXA",
        "colab_type": "text"
      },
      "source": [
        "Finding words with the highest frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6WoIN__5JF",
        "colab_type": "code",
        "outputId": "588afcb9-d9c6-40d5-b5b2-b3e4cb649224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 232292), (',', 221170), ('.', 189176), ('and', 125479), ('a', 124871), ('of', 115520), ('to', 107571), ('is', 87167), ('in', 70347), ('I', 61725), ('it', 61157), ('that', 56271), ('\"', 50959), (\"'s\", 49843), ('this', 48176), ('-', 42509), ('/><br', 40660), ('was', 39987), ('as', 34919), ('with', 34408)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atdLlWOl40F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # build the vocabulary\n",
        "# TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
        "# LABEL.build_vocab(train)\n",
        "\n",
        "# # make iterator for splits\n",
        "# train_iter, test_iter = data.BucketIterator.splits(\n",
        "#     (train, test), batch_size=3, device=0)\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "# train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "#     (train_data, val_data, test_data),\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWpf0u0itOMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}