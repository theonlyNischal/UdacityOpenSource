{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning of a Recurrent Neural Network for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you are going to learn how to train a Recurrent Neural Network (RNN) in a federated way with the purpose of *classifying* a person's surname to its most likely language of origin. \n",
    "\n",
    "\n",
    "We will train two Recurrent Neural Networks residing on two remote workers based on a dataset containing approximately 20.000 surnames from 18 languages of origin, and predict to which language a name belongs based on the name's spelling. \n",
    "\n",
    "A **character-level RNN** treats words as a series of characters - outputting a prediction and “hidden state” per character, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to. Hence the training process proceeds sequentially character-by-character through the different hidden layers.\n",
    "\n",
    "Following distributed training, the resulting models are going to be able to perform operations like the following ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "predict(model_pointers[\"bob\"], \"Qing\", alice) #alice is our worker\n",
    "\n",
    " Qing\n",
    "(-1.43) Korean\n",
    "(-1.74) Vietnamese\n",
    "(-2.18) Arabic\n",
    "\n",
    "predict(model_pointers[\"alice\"], \"Daniele\", alice)\n",
    "\n",
    " Daniele\n",
    "(-1.58) French\n",
    "(-2.04) Scottish\n",
    "(-2.07) Dutch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present example is inspired by an official Pytorch [tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html), which I ported to PySyft with  the purpose of learning a Recurrent Neural Network in a federated way.The present tutorial is self-contained, so there are no dependencies on external pieces of code apart from a few Python libraries.\n",
    "\n",
    "**RNN Tutorial's author**: Daniele Gadler. [@DanyEle](https://github.com/danyele) on Github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step: Dependencies!\n",
    "\n",
    "Make sure you have all the requires packages installed, or install them via the following command (assuming you didn't move the current Jupyter Notebook from its directory. After installing new packages, you may have to restart your Notebook Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask>=1.0.2 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: flask_socketio>=3.3.2 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 2)) (4.2.0)\n",
      "Requirement already satisfied: lz4>=2.1.6 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 3)) (2.1.10)\n",
      "Requirement already satisfied: msgpack>=0.6.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 6)) (0.21.3)\n",
      "Requirement already satisfied: tblib>=1.4.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: tf_encrypted!=0.5.7,>=0.5.4 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 8)) (0.5.8)\n",
      "Requirement already satisfied: torch==1.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: torchvision==0.3.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 10)) (0.3.0)\n",
      "Requirement already satisfied: websocket_client>=0.56.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 11)) (0.56.0)\n",
      "Requirement already satisfied: websockets>=7.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 12)) (8.0.1)\n",
      "Requirement already satisfied: zstd>=1.4.0.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from -r ../../../requirements.txt (line 13)) (1.4.0.0)\n",
      "Requirement already satisfied: click>=5.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from Flask>=1.0.2->-r ../../../requirements.txt (line 1)) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from Flask>=1.0.2->-r ../../../requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from Flask>=1.0.2->-r ../../../requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from Flask>=1.0.2->-r ../../../requirements.txt (line 1)) (0.15.5)\n",
      "Requirement already satisfied: python-socketio>=4.3.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from flask_socketio>=3.3.2->-r ../../../requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from scikit-learn>=0.21.0->-r ../../../requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from scikit-learn>=0.21.0->-r ../../../requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: tensorflow<2,>=1.12.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: six in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from torchvision==0.3.0->-r ../../../requirements.txt (line 10)) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from torchvision==0.3.0->-r ../../../requirements.txt (line 10)) (6.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from Jinja2>=2.10.1->Flask>=1.0.2->-r ../../../requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: python-engineio>=3.9.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from python-socketio>=4.3.0->flask_socketio>=3.3.2->-r ../../../requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (0.1.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (0.33.4)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (3.9.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.22.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/envs/pysyft/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->-r ../../../requirements.txt (line 8)) (3.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r \"../../../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 19:35:29.966765 4402099648 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0817 19:35:30.026469 4402099648 deprecation_wrapper.py:119] From /anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import syft as sy\n",
    "import pandas as pd\n",
    "import random\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "from syft.workers import WebsocketClientWorker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step: Data pre-processing and transformation\n",
    "\n",
    "We are going to train our neural network based on a dataset containing surnames from 18 languages of origin, so download the following dataset [LINK](https://download.pytorch.org/tutorial/data.zip) and extract it to the same directory of this Jupyter notebook. After you've extracted the data, you'll be able to read it in Python after initializing a few basic functions for parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the files in a certain path\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "#convert a string 's' in unicode format to ASCII format\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Czech.txt\n",
      "data/names/German.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/French.txt\n",
      "data/names/Irish.txt\n",
      "data/names/English.txt\n",
      "data/names/Spanish.txt\n",
      "data/names/Greek.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Polish.txt\n",
      "Amount of categories:18\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "#dictionary containing the nation as key and the names as values\n",
    "#Example: category_lines[\"italian\"] = [\"Abandonato\",\"Abatangelo\",\"Abatantuono\",...]\n",
    "category_lines = {}\n",
    "#List containing the different categories in the data\n",
    "all_categories = []\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    print(filename)\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines   \n",
    "    \n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"Amount of categories:\" + str(n_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to format the data so as to make it compliant with the format requested by PySyft and Pytorch. Firstly, we define a dataset class, specifying how batches ought to be extracted from the dataset in order for them to be assigned to the different workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    #Constructor is mandatory\n",
    "        def __init__(self, text, labels, transform=None):\n",
    "            self.data = text\n",
    "            self.targets = labels #categories\n",
    "            #self.to_torchtensor()\n",
    "            self.transform = transform\n",
    "        \n",
    "        def to_torchtensor(self):            \n",
    "            self.data = torch.from_numpy(self.text, requires_grad=True)\n",
    "            self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
    "        \n",
    "        def __len__(self):\n",
    "            #Mandatory\n",
    "            '''Returns:\n",
    "                    Length [int]: Length of Dataset/batches\n",
    "            '''\n",
    "            return len(self.data)\n",
    "    \n",
    "        def __getitem__(self, idx): \n",
    "            #Mandatory \n",
    "            \n",
    "            '''Returns:\n",
    "                     Data [Torch Tensor]: \n",
    "                     Target [ Torch Tensor]:\n",
    "            '''\n",
    "            sample = self.data[idx]\n",
    "            target = self.targets[idx]\n",
    "                    \n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "    \n",
    "            return sample,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list of arguments for our program. We will be needing most of them soon.\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 0.005\n",
    "        self.epochs = 10000\n",
    "        self.federate_after_n_batches = 15000\n",
    "        self.seed = 1\n",
    "        self.print_every = 200\n",
    "        self.plot_every = 100\n",
    "        self.use_cuda = False\n",
    "        \n",
    "args = Arguments()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to unwrap data samples so as to have them all in one single list instead of a dictionary,where different categories were addressed by key.From now onwards, **categories** will be the languages of origin (Y) and **names** will be the data points (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{split}\n",
       "names\\_list = [d_1,...,d_n]  \\\\\n",
       "\n",
       "category\\_list = [c_1,...,c_n] \n",
       "\\end{split}\n",
       "\n",
       "\n",
       "Where $n$ is the total amount of data points\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\begin{split}\n",
    "names\\_list = [d_1,...,d_n]  \\\\\n",
    "\n",
    "category\\_list = [c_1,...,c_n] \n",
    "\\end{split}\n",
    "\n",
    "\n",
    "Where $n$ is the total amount of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adsit', 'Ajdrna', 'Alt', 'Antonowitsch', 'Antonowitz', 'Bacon', 'Ballalatak', 'Ballaltick', 'Bartonova', 'Bastl', 'Baroch', 'Benesch', 'Betlach', 'Biganska', 'Bilek', 'Blahut', 'Blazek', 'Blazek', 'Blazejovsky']\n",
      "['Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech', 'Czech']\n",
      "\n",
      " \n",
      " Amount of data points loaded: 20074\n"
     ]
    }
   ],
   "source": [
    "#Set of names(X)\n",
    "names_list = []\n",
    "#Set of labels (Y)\n",
    "category_list = []\n",
    "\n",
    "#Convert into a list with corresponding label.\n",
    "\n",
    "for nation, names in category_lines.items():\n",
    "    #iterate over every single name\n",
    "    for name in names:\n",
    "        names_list.append(name)      #input data point\n",
    "        category_list.append(nation) #label\n",
    "        \n",
    "#let's see if it was successfully loaded. Each data sample(X) should have its own corresponding category(Y)\n",
    "print(names_list[1:20])\n",
    "print(category_list[1:20])\n",
    "\n",
    "print(\"\\n \\n Amount of data points loaded: \" + str(len(names_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to turn our categories into numbers, as PyTorch cannot really understand plain text\n",
    "\n",
    "For an example category: \"Greek\" ---> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wedekind', 'Weeber', 'Wegener', 'Wegner', 'Wehner', 'Wehunt', 'Weigand', 'Weiman', 'Weiner', 'Weiss']\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Assign an integer to every category\n",
    "categories_numerical = pd.factorize(category_list)[0]\n",
    "#Let's wrap our categories with a tensor, so that it can be loaded by LanguageDataset\n",
    "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
    "#Ready to be processed by torch.from_numpy in LanguageDataset\n",
    "categories_numpy = np.array(category_tensor)\n",
    "\n",
    "#Let's see a few resulting categories\n",
    "print(names_list[1200:1210])\n",
    "print(categories_numpy[1200:1210])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to turn every single character in each input line string into a vector, with a \"1\" marking the character present in that very character.\n",
    "\n",
    "For example, in the case of a single character, we have:\n",
    "\n",
    "\"a\" = array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n",
    "\n",
    "\n",
    "A word is just a vector of such character vectors: our Recurrent Neural Network will process every single character vector in the word, producing an output after passing through each of its hidden layers. \n",
    "\n",
    "This technique, involving the encoding of a word as a vector of character vectors, is known as *word embedding*, as we embed a word into a vector of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abl\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(3, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "    \n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters) #Daniele: len(max_line_size) was len(line)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    #Daniele: add blank elements over here\n",
    "    return tensor    \n",
    "    \n",
    "    \n",
    "    \n",
    "def list_strings_to_list_tensors(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensor(line)\n",
    "        lineNumpy = lineTensor.numpy()\n",
    "        lines_tensors.append(lineNumpy)\n",
    "        \n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors(names_list)\n",
    "\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now identify the longest word in the dataset, as all tensors need to have the same shape  in order to fit into a numpy array. So, we append vectors containing just \"0\"s into our words up to the maximum word size, such that all word embeddings have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abl\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "torch.Size([19, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_line_size = max(len(x) for x in lines_tensors)\n",
    "\n",
    "def lineToTensorFillEmpty(line, max_line_size):\n",
    "    tensor = torch.zeros(max_line_size, 1, n_letters) #notice the difference between this method and the previous one\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "        \n",
    "        #Vectors with (0,0,.... ,0) are placed where there are no characters\n",
    "    return tensor\n",
    "\n",
    "def list_strings_to_list_tensors_fill_empty(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
    "        lines_tensors.append(lineTensor)\n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)\n",
    "\n",
    "#Let's take a look at what a word now looks like\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20074, 19, 1, 57)\n",
      "(20074, 19, 57)\n"
     ]
    }
   ],
   "source": [
    "#And finally, from a list, we can create a numpy array with all our word embeddings having the same shape:\n",
    "array_lines_tensors = np.stack(lines_tensors)\n",
    "#However, such operation introduces one extra dimension (look at the dimension with index=2 having size '1')\n",
    "print(array_lines_tensors.shape)\n",
    "#Because that dimension just has size 1, we can get rid of it with the following function call\n",
    "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
    "print(array_lines_proper_dimension.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data unbalancing and batch randomization:\n",
    "You may have noticed that our dataset is strongly unbalanced and contains a lot of data points in the \"russian.txt\" dataset. However, we would still like to take a random batch during our training procedure at every iteration. In order to prevent our neural network from classifying a data point as always belonging to the \"Russian\" category, we first pick a random category and then select a data point from that category. To do that, we construct a dictionary mapping a certain category to the corresponding starting index in the list of data points (e.g.: lines). Afterwards, we will take a datapoint starting from the starting_index identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Czech': 0, 'German': 519, 'Arabic': 1243, 'Japanese': 3243, 'Chinese': 4234, 'Vietnamese': 4502, 'Russian': 4575, 'French': 13983, 'Irish': 14260, 'English': 14492, 'Spanish': 18160, 'Greek': 18458, 'Italian': 18661, 'Portuguese': 19370, 'Scottish': 19444, 'Dutch': 19544, 'Korean': 19841, 'Polish': 19935}\n"
     ]
    }
   ],
   "source": [
    "def find_start_index_per_category(category_list):\n",
    "    categories_start_index = {}\n",
    "    \n",
    "    #Initialize every category with an empty list\n",
    "    for category in all_categories:\n",
    "        categories_start_index[category] = []\n",
    "    \n",
    "    #Insert the start index of each category into the dictionary categories_start_index\n",
    "    #Example: \"Italian\" --> 203\n",
    "    #         \"Spanish\" --> 19776\n",
    "    last_category = None\n",
    "    i = 0\n",
    "    for name in names_list:\n",
    "        cur_category = category_list[i]\n",
    "        if(cur_category != last_category):\n",
    "            categories_start_index[cur_category] = i\n",
    "            last_category = cur_category\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    return(categories_start_index)\n",
    "\n",
    "categories_start_index = find_start_index_per_category(category_list)\n",
    "\n",
    "print(categories_start_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few functions to take a random index from from the dataset, so that we'll be able to select a random data point and a random category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    rand_value = random.randint(0, len(l) - 1)\n",
    "    return l[rand_value], rand_value\n",
    "\n",
    "\n",
    "def randomTrainingIndex():\n",
    "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
    "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
    "    line, rand_line_index = randomChoice(category_lines[category])\n",
    "    category_start_index = categories_start_index[category]\n",
    "    absolute_index = category_start_index + rand_line_index\n",
    "    return(absolute_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step: Model - Recurrent Neural Network\n",
    "Hey, I must admit that was indeed a lot of data preprocessing and transformation, but it was well worth it! \n",
    "We have defined almost all the function we'll be needing during the training procedure and our data is ready\n",
    "to be fed into the neural network, which we're creating now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Two hidden layers, based on simple linear layers\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "#Let's instantiate the neural network already:\n",
    "n_hidden = 128\n",
    "#Instantiate RNN\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "model = RNN(n_letters, n_hidden, n_categories).to(device)\n",
    "#The final softmax layer will produce a probability for each one of our 18 categories\n",
    "print(model)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's define our workers. You can either use remote workers or virtual workers\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "#alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
    "#bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "#charlie = sy.VirtualWorker(hook, id=\"charlie\") \n",
    "\n",
    "#workers_virtual = [alice, bob]\n",
    "\n",
    "#If you have your workers operating remotely, like on Raspberry PIs\n",
    "kwargs_websocket_alice = {\"host\": \"192.168.2.99\", \"hook\": hook}\n",
    "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
    "kwargs_websocket_bob = {\"host\": \"192.168.2.223\", \"hook\": hook}\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
    "workers_virtual = [alice, bob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_lines_proper_dimension = our data points(X)\n",
    "#categories_numpy = our labels (Y)\n",
    "langDataset =  LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
    "\n",
    "#assign the data points and the corresponding categories to workers.\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "            langDataset\n",
    "            .federate(workers_virtual),\n",
    "            batch_size=args.batch_size)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step - Model Training!\n",
    "\n",
    "\n",
    "It's now time to train our Recurrent Neural Network based on the processed data. To do that, we need to define a few more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def fed_avg_every_n_iters(model_pointers, iter, federate_after_n_batches):\n",
    "        models_local = {}\n",
    "        \n",
    "        if(iter % args.federate_after_n_batches == 0):\n",
    "            for worker_name, model_pointer in model_pointers.items():\n",
    "#                #need to assign the model to the worker it belongs to.\n",
    "                models_local[worker_name] = model_pointer.copy().get()\n",
    "            model_avg = utils.federated_avg(models_local)\n",
    "           \n",
    "            for worker in workers_virtual:\n",
    "                model_copied_avg = model_avg.copy()\n",
    "                model_ptr = model_copied_avg.send(worker) \n",
    "                model_pointers[worker.id] = model_ptr\n",
    "                \n",
    "        return(model_pointers)     \n",
    "\n",
    "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
    "    #get the right initialized model\n",
    "    model_ptr = model_pointers[line_single.location.id]   \n",
    "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
    "    line_reshaped, category_single = line_reshaped.to(device), category_single.to(device)\n",
    "    #Firstly, initialize hidden layer\n",
    "    hidden_init = model_ptr.initHidden() \n",
    "    #And now zero grad the model\n",
    "    model_ptr.zero_grad()\n",
    "    hidden_ptr = hidden_init.send(line_single.location)\n",
    "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
    "    #now need to perform forward passes\n",
    "    for i in range(amount_lines_non_zero): \n",
    "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
    "    criterion = nn.NLLLoss()   \n",
    "    loss = criterion(output, category_single) \n",
    "    loss.backward()\n",
    "    \n",
    "    model_got = model_ptr.get() \n",
    "    \n",
    "    #Perform model weights' updates    \n",
    "    for param in model_got.parameters():\n",
    "        param.data.add_(-args.learning_rate, param.grad.data)\n",
    "        \n",
    "        \n",
    "    model_sent = model_got.send(line_single.location.id)\n",
    "    model_pointers[line_single.location.id] = model_sent\n",
    "    \n",
    "    return(model_pointers, loss, output)\n",
    "            \n",
    "  \n",
    "    \n",
    "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
    "    current_loss = 0\n",
    "    all_losses = []    \n",
    "    \n",
    "    model_pointers = {}\n",
    "    \n",
    "    #Send the initialized model to every single worker just before the training procedure starts\n",
    "    for worker in workers_virtual:\n",
    "        model_copied = model.copy()\n",
    "        model_ptr = model_copied.send(worker) \n",
    "        model_pointers[worker.id] = model_ptr\n",
    "\n",
    "    #extract a random element from the list and perform training on it\n",
    "    for iter in range(1, n_iters + 1):        \n",
    "        random_index = randomTrainingIndex()\n",
    "        line_single, category_single = list_federated_train_loader[random_index]\n",
    "        #print(category_single.copy().get())\n",
    "        line_name = names_list[random_index]\n",
    "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
    "        #model_pointers = fed_avg_every_n_iters(model_pointers, iter, args.federate_after_n_batches)\n",
    "        #Update the current loss a\n",
    "        loss_got = loss.get().item() \n",
    "        current_loss += loss_got\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "             \n",
    "        if(iter % print_every == 0):\n",
    "            output_got = output.get()  #Without copy()\n",
    "            guess, guess_i = categoryFromOutput(output_got)\n",
    "            category = all_categories[category_single.copy().get().item()]\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss_got, line_name, guess, correct))\n",
    "    return(all_losses, model_pointers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the defined randomization process to work, we need to wrap the data points and categories into a list, from that we're going to take a batch at a random index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of batches for the workers...\n"
     ]
    }
   ],
   "source": [
    "#This may take a few seconds to complete.\n",
    "print(\"Generating list of batches for the workers...\")\n",
    "list_federated_train_loader = list(federated_train_loader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally,let's launch our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 2% (3m 24s) 2.8717 Jamussa / Polish ✗ (Greek)\n",
      "400 4% (6m 42s) 2.7891 Alt / Chinese ✗ (Czech)\n",
      "600 6% (9m 59s) 2.8131 Mckay / Irish ✗ (Scottish)\n",
      "800 8% (13m 19s) 2.7211 Sin / Polish ✗ (Korean)\n",
      "1000 10% (16m 38s) 2.3581 Trinh / Chinese ✗ (Vietnamese)\n",
      "1200 12% (19m 58s) 2.2701 Okanao / Arabic ✗ (Japanese)\n",
      "1400 14% (23m 14s) 2.5655 Hamilton / Scottish ✓\n",
      "1600 16% (26m 34s) 2.6941 Nunes / Greek ✗ (Portuguese)\n",
      "1800 18% (29m 59s) 2.5181 Rosario / Spanish ✗ (Portuguese)\n",
      "2000 20% (33m 21s) 2.4670 Mateus / Greek ✗ (Portuguese)\n",
      "2200 22% (36m 43s) 3.0153 Tsai  / Spanish ✗ (Korean)\n",
      "2400 24% (40m 4s) 3.3842 Cresswell / Irish ✗ (English)\n",
      "2600 26% (43m 28s) 2.5066 Bonnet / Irish ✗ (French)\n",
      "2800 28% (46m 49s) 1.7556 Totah / Czech ✗ (Arabic)\n",
      "3000 30% (50m 13s) 1.6116 Sui / Chinese ✓\n",
      "3200 32% (53m 41s) 3.2945 Tets / Greek ✗ (Russian)\n",
      "3400 34% (57m 24s) 2.4125 Manos / Scottish ✗ (Greek)\n",
      "3600 36% (61m 22s) 1.9158 Steffen / Czech ✗ (German)\n",
      "3800 38% (65m 7s) 1.5111 Son / Korean ✓\n",
      "4000 40% (68m 32s) 1.6698 Bang / Korean ✓\n",
      "4200 42% (71m 54s) 2.0176 Pinheiro / Polish ✗ (Portuguese)\n",
      "4400 44% (75m 17s) 0.3490 Calogerakis / Greek ✓\n",
      "4600 46% (78m 46s) 2.3684 Daniel / Irish ✗ (English)\n",
      "4800 48% (82m 5s) 1.8619 Deushi / Arabic ✗ (Japanese)\n",
      "5000 50% (85m 29s) 2.4137 Aldred / Spanish ✗ (English)\n",
      "5200 52% (88m 56s) 2.8667 Goto / Chinese ✗ (Japanese)\n",
      "5400 54% (92m 37s) 2.6098 Eoin / Korean ✗ (Irish)\n",
      "5600 56% (96m 2s) 1.5684 Oursler / Czech ✗ (German)\n",
      "5800 57% (99m 26s) 2.0652 Rademakers / Greek ✗ (Dutch)\n",
      "6000 60% (102m 49s) 0.7673 Woo / Korean ✓\n",
      "6200 62% (106m 13s) 2.3699 Yonai / Arabic ✗ (Japanese)\n",
      "6400 64% (109m 38s) 1.6237 Dasios / Greek ✓\n",
      "6600 66% (113m 5s) 2.7083 Arkins / Portuguese ✗ (English)\n",
      "6800 68% (116m 42s) 3.5584 Gravari / Italian ✗ (Greek)\n",
      "7000 70% (120m 34s) 2.1956 Kelly / English ✗ (Scottish)\n",
      "7200 72% (123m 58s) 3.1656 Gajos / Portuguese ✗ (Polish)\n",
      "7400 74% (127m 23s) 0.5196 Yim / Korean ✓\n",
      "7600 76% (130m 52s) 0.4545 Xing / Chinese ✓\n",
      "7800 78% (134m 20s) 2.3319 Poingdestre / Greek ✗ (French)\n",
      "8000 80% (137m 43s) 1.9313 Kokan / Czech ✗ (Japanese)\n",
      "8200 82% (141m 3s) 1.2661 Le / Chinese ✗ (Vietnamese)\n",
      "8400 84% (144m 30s) 2.2196 Oliver / Dutch ✗ (French)\n",
      "8600 86% (148m 4s) 2.2463 Blythe / French ✗ (English)\n",
      "8800 88% (151m 32s) 2.3357 Arendonk / Polish ✗ (Dutch)\n",
      "9000 90% (155m 5s) 1.0508 Zou / Vietnamese ✗ (Chinese)\n",
      "9200 92% (158m 48s) 1.7407 Ukiyo / German ✗ (Japanese)\n",
      "9400 94% (162m 38s) 1.7899 Allan / Irish ✗ (Scottish)\n",
      "9600 96% (167m 4s) 2.5512 Sgro / Korean ✗ (Italian)\n",
      "9800 98% (170m 54s) 2.1999 Hwang / English ✗ (Korean)\n",
      "10000 100% (174m 47s) 2.0826 Duffield / Scottish ✗ (English)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, args.plot_every, args.federate_after_n_batches, list_federated_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x640649550>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's plot the loss we got during the training procedure\n",
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epochs (100s)')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step - Predict!\n",
    "Great! We have successfully created our two models for bob and alice in parallel using federated learning! I experimented with federated averaging of the two models, but it turned out that for a batch size of 1, as in the present case, the model loss was diverging. Let's try using our models for prediction now, shall we? This is the final reward for our endeavours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.03) Arabic\n",
      "(-1.37) Czech\n",
      "(-1.77) Japanese\n"
     ]
    }
   ],
   "source": [
    "input_line = \"Daniele\"\n",
    "model_remote = model_pointers[\"alice\"]\n",
    "line_tensor = lineToTensor(input_line)\n",
    "line_tensor\n",
    "line_remote = line_tensor.send(alice)\n",
    "hidden = model_remote.initHidden()\n",
    "hidden_remote = hidden.copy().send(alice)\n",
    "hidden_remote\n",
    "with torch.no_grad():\n",
    "    for i in range(line_remote.shape[0]):\n",
    "        output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
    "        \n",
    "n_predictions = 3\n",
    "topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
    "topv\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_line, worker, n_predictions=3):\n",
    "    model = model.copy().get()\n",
    "    print('\\n> %s' % input_line)\n",
    "    model_remote = model.send(worker)\n",
    "    line_tensor = lineToTensor(input_line)\n",
    "    line_remote = line_tensor.copy().send(worker)\n",
    "    #line_tensor = lineToTensor(input_line)\n",
    "    #output = evaluate(model, line_remote)\n",
    "    # Get top N categories\n",
    "    hidden = model_remote.initHidden()\n",
    "    hidden_remote = hidden.copy().send(worker)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i in range(line_remote.shape[0]):\n",
    "            output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
    "        \n",
    "    topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the different models learned may perform different predictions, based on the data that was shown to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Qing\n",
      "(-0.45) Chinese\n",
      "(-1.31) Vietnamese\n",
      "(-3.29) Arabic\n"
     ]
    }
   ],
   "source": [
    "predict(model_pointers[\"alice\"], \"Qing\", alice) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may try experimenting with this example right now, for example by increasing or decreasing the amount of epochs and seeing how the two models perform. You may also try to de-commenting the part about federating averaging and check the new resulting loss function. There can be lots of other optimizations we may think of as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
