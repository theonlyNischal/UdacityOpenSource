{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearningOnRaspberryPis_Windows.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2oW_FJYibER",
        "outputId": "803ed21a-9643-4641-bab5-00b31aefe303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip3 list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package            Version    \n",
            "------------------ -----------\n",
            "attrs              19.1.0     \n",
            "backcall           0.1.0      \n",
            "bleach             3.1.0      \n",
            "Click              7.0        \n",
            "colorama           0.4.1      \n",
            "cycler             0.10.0     \n",
            "decorator          4.4.0      \n",
            "defusedxml         0.6.0      \n",
            "entrypoints        0.3        \n",
            "Flask              1.1.1      \n",
            "Flask-SocketIO     4.2.1      \n",
            "ipykernel          5.1.2      \n",
            "ipython            7.7.0      \n",
            "ipython-genutils   0.2.0      \n",
            "ipywidgets         7.5.1      \n",
            "itsdangerous       1.1.0      \n",
            "jedi               0.15.1     \n",
            "Jinja2             2.10.1     \n",
            "json5              0.8.5      \n",
            "jsonschema         3.0.2      \n",
            "jupyter            1.0.0      \n",
            "jupyter-client     5.3.1      \n",
            "jupyter-console    6.0.0      \n",
            "jupyter-core       4.5.0      \n",
            "jupyterlab         1.0.5      \n",
            "jupyterlab-server  1.0.0      \n",
            "kiwisolver         1.1.0      \n",
            "lz4                2.1.10     \n",
            "MarkupSafe         1.1.1      \n",
            "matplotlib         3.1.1      \n",
            "mistune            0.8.4      \n",
            "msgpack            0.6.1      \n",
            "nbconvert          5.6.0      \n",
            "nbformat           4.4.0      \n",
            "notebook           6.0.0      \n",
            "numpy              1.17.0     \n",
            "pandas             0.25.0     \n",
            "pandocfilters      1.4.2      \n",
            "parso              0.5.1      \n",
            "pickleshare        0.7.5      \n",
            "Pillow             6.1.0      \n",
            "pip                19.0.3     \n",
            "prometheus-client  0.7.1      \n",
            "prompt-toolkit     2.0.9      \n",
            "Pygments           2.4.2      \n",
            "pyparsing          2.4.2      \n",
            "pyrsistent         0.15.4     \n",
            "python-dateutil    2.8.0      \n",
            "python-engineio    3.9.3      \n",
            "python-socketio    4.3.1      \n",
            "pytz               2019.2     \n",
            "pywinpty           0.5.5      \n",
            "PyYAML             5.1.2      \n",
            "pyzmq              18.1.0     \n",
            "qtconsole          4.5.2      \n",
            "Send2Trash         1.5.0      \n",
            "setuptools         40.8.0     \n",
            "six                1.12.0     \n",
            "syft               0.1.13a1   \n",
            "terminado          0.8.2      \n",
            "testpath           0.4.2      \n",
            "torch              1.0.0      \n",
            "torchvision        0.2.2.post3\n",
            "tornado            6.0.3      \n",
            "traitlets          4.3.2      \n",
            "wcwidth            0.1.7      \n",
            "webencodings       0.5.1      \n",
            "websocket-client   0.56.0     \n",
            "websockets         8.0.2      \n",
            "Werkzeug           0.15.5     \n",
            "widgetsnbextension 3.5.1      \n",
            "zstd               1.4.1.0    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.2.2 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMf1JulmrMSU",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import syft as sy\n",
        "import pandas as pd\n",
        "import random\n",
        "from syft.frameworks.torch.federated import utils\n",
        "\n",
        "from syft.workers import WebsocketClientWorker\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SfRtGt8GFlTO",
        "colab": {}
      },
      "source": [
        "#Load all the files in a certain path\n",
        "def findFiles(path):\n",
        "    return glob.glob(path)\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "#convert a string 's' in unicode format to ASCII format\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join (\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "01Z3MAOQFq-z",
        "outputId": "5fdc6c0e-e8f6-4878-a0fc-68f5bb892454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "data_path = './data/names/*.txt'\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "for filename in findFiles(data_path):\n",
        "    print(filename)\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines   \n",
        "    \n",
        "n_categories = len(all_categories)\n",
        "\n",
        "print(\"Amount of categories:\" + str(n_categories))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./data/names\\Arabic.txt\n",
            "./data/names\\Chinese.txt\n",
            "./data/names\\Czech.txt\n",
            "./data/names\\Dutch.txt\n",
            "./data/names\\English.txt\n",
            "./data/names\\French.txt\n",
            "./data/names\\German.txt\n",
            "./data/names\\Greek.txt\n",
            "./data/names\\Irish.txt\n",
            "./data/names\\Italian.txt\n",
            "./data/names\\Japanese.txt\n",
            "./data/names\\Korean.txt\n",
            "./data/names\\Polish.txt\n",
            "./data/names\\Portuguese.txt\n",
            "./data/names\\Russian.txt\n",
            "./data/names\\Scottish.txt\n",
            "./data/names\\Spanish.txt\n",
            "./data/names\\Vietnamese.txt\n",
            "Amount of categories:18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BbCNl5vOG7Q0",
        "colab": {}
      },
      "source": [
        "class LanguageDataset(Dataset):\n",
        "    #Constructor is mandatory\n",
        "        def __init__(self, text, labels, transform=None):\n",
        "            self.data = text\n",
        "            self.targets = labels #categories\n",
        "            #self.to_torchtensor()\n",
        "            self.transform = transform\n",
        "        \n",
        "        def to_torchtensor(self):            \n",
        "            self.data = torch.from_numpy(self.text, requires_grad=True)\n",
        "            self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
        "        \n",
        "        def __len__(self):\n",
        "            #Mandatory\n",
        "            '''Returns:\n",
        "                    Length [int]: Length of Dataset/batches\n",
        "            '''\n",
        "            return len(self.data)\n",
        "    \n",
        "        def __getitem__(self, idx): \n",
        "            #Mandatory \n",
        "            \n",
        "            '''Returns:\n",
        "                     Data [Torch Tensor]: \n",
        "                     Target [ Torch Tensor]:\n",
        "            '''\n",
        "            sample = self.data[idx]\n",
        "            target = self.targets[idx]\n",
        "                    \n",
        "            if self.transform:\n",
        "                sample = self.transform(sample)\n",
        "    \n",
        "            return sample,target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eQba15YQHYR2",
        "colab": {}
      },
      "source": [
        "#The list of arguments for our program. We will be needing most of them soon.\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 1\n",
        "        self.learning_rate = 0.005\n",
        "        self.epochs = 10000\n",
        "        self.federate_after_n_batches = 15000\n",
        "        self.seed = 1\n",
        "        self.print_every = 200\n",
        "        self.plot_every = 100\n",
        "        self.use_cuda = False\n",
        "        \n",
        "args = Arguments()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u34QkoBkHlb0",
        "outputId": "8311180e-e4ca-4dc8-a48d-8b932204716f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%latex\n",
        "\n",
        "\\begin{split}\n",
        "names\\_list = [d_1,...,d_n]  \\\\\n",
        "\n",
        "category\\_list = [c_1,...,c_n] \n",
        "\\end{split}\n",
        "\n",
        "\n",
        "Where $n$ is the total amount of data points"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/latex": "\n\\begin{split}\nnames\\_list = [d_1,...,d_n]  \\\\\n\ncategory\\_list = [c_1,...,c_n] \n\\end{split}\n\n\nWhere $n$ is the total amount of data points\n",
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0aZBnZEHpfH",
        "outputId": "c485476b-d3d3-4b14-f1c1-b6443eefc7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Set of names(X)\n",
        "names_list = []\n",
        "#Set of labels (Y)\n",
        "category_list = []\n",
        "\n",
        "#Convert into a list with corresponding label.\n",
        "\n",
        "for nation, names in category_lines.items():\n",
        "    #iterate over every single name\n",
        "    for name in names:\n",
        "        names_list.append(name)      #input data point\n",
        "        category_list.append(nation) #label\n",
        "        \n",
        "#let's see if it was successfully loaded. Each data sample(X) should have its own corresponding category(Y)\n",
        "print(names_list[1:20])\n",
        "print(category_list[1:20])\n",
        "\n",
        "print(\"\\n \\n Amount of data points loaded: \" + str(len(names_list)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Nahas', 'Daher', 'Gerges', 'Nazari', 'Maalouf', 'Gerges', 'Naifeh', 'Guirguis', 'Baba', 'Sabbagh', 'Attia', 'Tahan', 'Haddad', 'Aswad', 'Najjar', 'Dagher', 'Maloof', 'Isa', 'Asghar']\n",
            "['Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic']\n",
            "\n",
            " \n",
            " Amount of data points loaded: 20074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z5QXRytdH2NQ",
        "outputId": "bbf5de9a-0e8a-4123-c023-794b8a409688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Assign an integer to every category\n",
        "categories_numerical = pd.factorize(category_list)[0]\n",
        "#Let's wrap our categories with a tensor, so that it can be loaded by LanguageDataset\n",
        "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
        "#Ready to be processed by torch.from_numpy in LanguageDataset\n",
        "categories_numpy = np.array(category_tensor)\n",
        "\n",
        "#Let's see a few resulting categories\n",
        "print(names_list[1200:1210])\n",
        "print(categories_numpy[1200:1210])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Said', 'Guirguis', 'Nader', 'Harb', 'Atiyeh', 'Zogby', 'Basara', 'Nassar', 'Kalb', 'Khoury']\n",
            "[0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-uFEKcRYH8np",
        "outputId": "bc9db229-132f-402e-b69c-59c47099e5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "    \n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters) #Daniele: len(max_line_size) was len(line)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    #Daniele: add blank elements over here\n",
        "    return tensor    \n",
        "    \n",
        "    \n",
        "    \n",
        "def list_strings_to_list_tensors(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensor(line)\n",
        "        lineNumpy = lineTensor.numpy()\n",
        "        lines_tensors.append(lineNumpy)\n",
        "        \n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors(names_list)\n",
        "\n",
        "print(names_list[0])\n",
        "print(lines_tensors[0])\n",
        "print(lines_tensors[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Khoury\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "(6, 1, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y575umqqJVI5",
        "outputId": "888a24cc-6638-4a79-cce8-24b246a6aa8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "max_line_size = max(len(x) for x in lines_tensors)\n",
        "\n",
        "def lineToTensorFillEmpty(line, max_line_size):\n",
        "    tensor = torch.zeros(max_line_size, 1, n_letters) #notice the difference between this method and the previous one\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "        \n",
        "        #Vectors with (0,0,.... ,0) are placed where there are no characters\n",
        "    return tensor\n",
        "\n",
        "def list_strings_to_list_tensors_fill_empty(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
        "        lines_tensors.append(lineTensor)\n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)\n",
        "\n",
        "#Let's take a look at what a word now looks like\n",
        "print(names_list[0])\n",
        "print(lines_tensors[0])\n",
        "print(lines_tensors[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Khoury\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "torch.Size([19, 1, 57])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDS_xjBlIAax",
        "outputId": "960956d6-666a-43db-eb45-8a81ac5e578c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#And finally, from a list, we can create a numpy array with all our word embeddings having the same shape:\n",
        "array_lines_tensors = np.stack(lines_tensors)\n",
        "#However, such operation introduces one extra dimension (look at the dimension with index=2 having size '1')\n",
        "print(array_lines_tensors.shape)\n",
        "#Because that dimension just has size 1, we can get rid of it with the following function call\n",
        "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
        "print(array_lines_proper_dimension.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20074, 19, 1, 57)\n",
            "(20074, 19, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y1UD93dXIVhg",
        "outputId": "cd46650c-c45a-4b96-fce2-9b0b6d211c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "def find_start_index_per_category(category_list):\n",
        "    categories_start_index = {}\n",
        "    \n",
        "    #Initialize every category with an empty list\n",
        "    for category in all_categories:\n",
        "        categories_start_index[category] = []\n",
        "    \n",
        "    #Insert the start index of each category into the dictionary categories_start_index\n",
        "    #Example: \"Italian\" --> 203\n",
        "    #         \"Spanish\" --> 19776\n",
        "    last_category = None\n",
        "    i = 0\n",
        "    for name in names_list:\n",
        "        cur_category = category_list[i]\n",
        "        if(cur_category != last_category):\n",
        "            categories_start_index[cur_category] = i\n",
        "            last_category = cur_category\n",
        "        \n",
        "        i = i + 1\n",
        "        \n",
        "    return(categories_start_index)\n",
        "\n",
        "categories_start_index = find_start_index_per_category(category_list)\n",
        "\n",
        "print(categories_start_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Arabic': 0, 'Chinese': 2000, 'Czech': 2268, 'Dutch': 2787, 'English': 3084, 'French': 6752, 'German': 7029, 'Greek': 7753, 'Irish': 7956, 'Italian': 8188, 'Japanese': 8897, 'Korean': 9888, 'Polish': 9982, 'Portuguese': 10121, 'Russian': 10195, 'Scottish': 19603, 'Spanish': 19703, 'Vietnamese': 20001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlk_lLPSJ0n-",
        "colab": {}
      },
      "source": [
        "def randomChoice(l):\n",
        "    rand_value = random.randint(0, len(l) - 1)\n",
        "    return l[rand_value], rand_value\n",
        "\n",
        "\n",
        "def randomTrainingIndex():\n",
        "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
        "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
        "    line, rand_line_index = randomChoice(category_lines[category])\n",
        "    category_start_index = categories_start_index[category]\n",
        "    absolute_index = category_start_index + rand_line_index\n",
        "    return(absolute_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSZ3_EKmJ9v6",
        "colab": {}
      },
      "source": [
        "## 3. Step: Model - Recurrent Neural Network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dnElNc76KQrs",
        "outputId": "cffc819c-3858-47b2-b79c-d11004454f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#Two hidden layers, based on simple linear layers\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "#Let's instantiate the neural network already:\n",
        "n_hidden = 128\n",
        "#Instantiate RNN\n",
        "\n",
        "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
        "model = RNN(n_letters, n_hidden, n_categories).to(device)\n",
        "#The final softmax layer will produce a probability for each one of our 18 categories\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
            "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nPIjPtnuKmc-",
        "colab": {}
      },
      "source": [
        "#Now let's define our workers. You can either use remote workers or virtual workers\n",
        "\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "\n",
        "#alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
        "#bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
        "#charlie = sy.VirtualWorker(hook, id=\"charlie\") \n",
        "\n",
        "#workers_virtual = [alice, bob]\n",
        "\n",
        "#If you have your workers operating remotely, like on Raspberry PIs\n",
        "my_ip=\"YOUR_IP\"\n",
        "kwargs_websocket_alice = {\"host\": my_ip, \"hook\": hook}\n",
        "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
        "kwargs_websocket_bob = {\"host\": my_ip, \"hook\": hook}\n",
        "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
        "workers_virtual = [alice, bob]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTjDBCOkKxG8",
        "colab": {}
      },
      "source": [
        "#array_lines_proper_dimension = our data points(X)\n",
        "#categories_numpy = our labels (Y)\n",
        "langDataset =  LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
        "\n",
        "#assign the data points and the corresponding categories to workers.\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "            langDataset\n",
        "            .federate(workers_virtual),\n",
        "            batch_size=args.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eZTu8CPEK1W-",
        "colab": {}
      },
      "source": [
        "## 4. Step - Model Training!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G7svbudkK6aG",
        "colab": {}
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def fed_avg_every_n_iters(model_pointers, iter, federate_after_n_batches):\n",
        "        models_local = {}\n",
        "        \n",
        "        if(iter % args.federate_after_n_batches == 0):\n",
        "            for worker_name, model_pointer in model_pointers.items():\n",
        "#                #need to assign the model to the worker it belongs to.\n",
        "                models_local[worker_name] = model_pointer.copy().get()\n",
        "            model_avg = utils.federated_avg(models_local)\n",
        "           \n",
        "            for worker in workers_virtual:\n",
        "                model_copied_avg = model_avg.copy()\n",
        "                model_ptr = model_copied_avg.send(worker) \n",
        "                model_pointers[worker.id] = model_ptr\n",
        "                \n",
        "        return(model_pointers)     \n",
        "\n",
        "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
        "    #get the right initialized model\n",
        "    model_ptr = model_pointers[line_single.location.id]   \n",
        "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
        "    line_reshaped, category_single = line_reshaped.to(device), category_single.to(device)\n",
        "    #Firstly, initialize hidden layer\n",
        "    hidden_init = model_ptr.initHidden() \n",
        "    #And now zero grad the model\n",
        "    model_ptr.zero_grad()\n",
        "    hidden_ptr = hidden_init.send(line_single.location)\n",
        "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
        "    #now need to perform forward passes\n",
        "    for i in range(amount_lines_non_zero): \n",
        "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
        "    criterion = nn.NLLLoss()   \n",
        "    loss = criterion(output, category_single) \n",
        "    loss.backward()\n",
        "    \n",
        "    model_got = model_ptr.get() \n",
        "    \n",
        "    #Perform model weights' updates    \n",
        "    for param in model_got.parameters():\n",
        "        param.data.add_(-args.learning_rate, param.grad.data)\n",
        "        \n",
        "        \n",
        "    model_sent = model_got.send(line_single.location.id)\n",
        "    model_pointers[line_single.location.id] = model_sent\n",
        "    \n",
        "    return(model_pointers, loss, output)\n",
        "            \n",
        "  \n",
        "    \n",
        "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
        "    current_loss = 0\n",
        "    all_losses = []    \n",
        "    \n",
        "    model_pointers = {}\n",
        "    \n",
        "    #Send the initialized model to every single worker just before the training procedure starts\n",
        "    for worker in workers_virtual:\n",
        "        model_copied = model.copy()\n",
        "        model_ptr = model_copied.send(worker) \n",
        "        model_pointers[worker.id] = model_ptr\n",
        "\n",
        "    #extract a random element from the list and perform training on it\n",
        "    for iter in range(1, n_iters + 1):        \n",
        "        random_index = randomTrainingIndex()\n",
        "        line_single, category_single = list_federated_train_loader[random_index]\n",
        "        #print(category_single.copy().get())\n",
        "        line_name = names_list[random_index]\n",
        "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
        "        #model_pointers = fed_avg_every_n_iters(model_pointers, iter, args.federate_after_n_batches)\n",
        "        #Update the current loss a\n",
        "        loss_got = loss.get().item() \n",
        "        current_loss += loss_got\n",
        "        \n",
        "        if iter % plot_every == 0:\n",
        "            all_losses.append(current_loss / plot_every)\n",
        "            current_loss = 0\n",
        "             \n",
        "        if(iter % print_every == 0):\n",
        "            output_got = output.get()  #Without copy()\n",
        "            guess, guess_i = categoryFromOutput(output_got)\n",
        "            category = all_categories[category_single.copy().get().item()]\n",
        "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss_got, line_name, guess, correct))\n",
        "    return(all_losses, model_pointers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGole52rK-mT",
        "outputId": "22ba8e6d-6435-4b65-e325-df9914c3f872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#This may take a few seconds to complete.\n",
        "print(\"Generating list of batches for the workers...\")\n",
        "list_federated_train_loader = list(federated_train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating list of batches for the workers...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyE8mZl9LA1j",
        "outputId": "6ef8285c-0ce1-499f-8227-b8734688628f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "start = time.time()\n",
        "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, args.plot_every, args.federate_after_n_batches, list_federated_train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200 2% (5m 29s) 2.9391 Morcos / Dutch ✗ (Arabic)\n",
            "400 4% (11m 28s) 2.7829 Rosa / German ✗ (Italian)\n",
            "600 6% (17m 10s) 2.7406 Gilmour / German ✗ (English)\n",
            "800 8% (23m 1s) 2.6871 Khoo / Chinese ✓\n",
            "1000 10% (28m 14s) 2.6200 Koizumi / Japanese ✓\n",
            "1200 12% (33m 16s) 2.6039 Deng / Chinese ✓\n",
            "1400 14% (38m 39s) 1.8089 Bautista / Scottish ✗ (Spanish)\n",
            "1600 16% (43m 55s) 1.9574 Nunes / Spanish ✗ (Portuguese)\n",
            "1800 18% (48m 53s) 2.5664 Burt / Dutch ✗ (English)\n",
            "2000 20% (53m 59s) 2.9513 Youn / English ✗ (Korean)\n",
            "2200 22% (59m 34s) 2.7775 Mcnally / Greek ✗ (English)\n",
            "2400 24% (65m 15s) 1.4349 Alves / Portuguese ✓\n",
            "2600 26% (70m 56s) 2.5769 Martin / English ✗ (French)\n",
            "2800 28% (76m 26s) 1.9034 Peeters / Dutch ✓\n",
            "3000 30% (82m 3s) 2.4535 Lew / German ✗ (Chinese)\n",
            "3200 32% (87m 38s) 2.1623 Tosi / Italian ✓\n",
            "3400 34% (92m 41s) 1.8401 Gomes / Scottish ✗ (Portuguese)\n",
            "3600 36% (97m 42s) 2.6528 Pfenning / Dutch ✗ (German)\n",
            "3800 38% (103m 52s) 2.0320 Paulis / Greek ✗ (Dutch)\n",
            "4000 40% (108m 38s) 1.1929 Chau / Vietnamese ✓\n",
            "4200 42% (113m 19s) 2.1259 Yeoman / German ✗ (English)\n",
            "4400 44% (117m 58s) 2.6850 Attrill / German ✗ (English)\n",
            "4600 46% (122m 46s) 1.3127 Onoprienko / Russian ✓\n",
            "4800 48% (127m 35s) 3.1633 Adamou / Japanese ✗ (Greek)\n",
            "5000 50% (132m 13s) 1.5418 Lefebvre / French ✓\n",
            "5200 52% (136m 41s) 3.1002 Czajka / Japanese ✗ (Polish)\n",
            "5400 54% (141m 57s) 2.5674 O'Halloran / Greek ✗ (Irish)\n",
            "5600 56% (146m 45s) 3.0844 Townsley / Italian ✗ (English)\n",
            "5800 57% (151m 30s) 1.1878 Song / Chinese ✗ (Korean)\n",
            "6000 60% (156m 12s) 3.1841 Lama / Korean ✗ (Italian)\n",
            "6200 62% (160m 57s) 2.4656 Simon / Arabic ✗ (French)\n",
            "6400 64% (165m 35s) 1.8678 Alberghini / Italian ✓\n",
            "6600 66% (170m 22s) 2.4334 Gerges / Dutch ✗ (Arabic)\n",
            "6800 68% (175m 5s) 0.4312 Phi / Vietnamese ✓\n",
            "7000 70% (179m 47s) 2.0810 Ruadhain / Dutch ✗ (Irish)\n",
            "7200 72% (184m 41s) 1.8331 Tailler / French ✓\n",
            "7400 74% (189m 31s) 2.3329 Bartonova / Italian ✗ (Czech)\n",
            "7600 76% (194m 16s) 0.6225 Batchelis / Russian ✓\n",
            "7800 78% (199m 9s) 1.9825 Niadh / Arabic ✗ (Irish)\n",
            "8000 80% (204m 11s) 2.7903 Kett / Korean ✗ (English)\n",
            "8200 82% (209m 11s) 1.6786 Raneri / Italian ✓\n",
            "8400 84% (213m 45s) 1.6833 Portner / German ✓\n",
            "8600 86% (218m 29s) 3.4946 Jmaev / Vietnamese ✗ (Russian)\n",
            "8800 88% (223m 10s) 0.5598 Thuy / Vietnamese ✓\n",
            "9000 90% (227m 53s) 3.3617 Rana / Korean ✗ (Italian)\n",
            "9200 92% (232m 52s) 1.4573 Segawa / Japanese ✓\n",
            "9400 94% (239m 53s) 2.1548 Emmanuel / German ✗ (English)\n",
            "9600 96% (244m 56s) 0.9693 Malinowski / Polish ✓\n",
            "9800 98% (250m 8s) 4.0732 Gwang  / Arabic ✗ (Korean)\n",
            "10000 100% (255m 17s) 1.6932 Wiesner / Czech ✓\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iDjZMoPHLEsc",
        "outputId": "85f252c9-cbfa-4efc-d256-22752d83c572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#Let's plot the loss we got during the training procedure\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Epochs (100s)')\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x2b184a39748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0cchH6PLG-N",
        "colab": {}
      },
      "source": [
        "## 5. Step - Predict!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlAAnsgWLKAV",
        "outputId": "19ff19f7-0528-43d2-fe98-97b9208d3b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "input_line = \"Daniele\"\n",
        "model_remote = model_pointers[\"alice\"]\n",
        "line_tensor = lineToTensor(input_line)\n",
        "line_tensor\n",
        "line_remote = line_tensor.send(alice)\n",
        "hidden = model_remote.initHidden()\n",
        "hidden_remote = hidden.copy().send(alice)\n",
        "hidden_remote\n",
        "with torch.no_grad():\n",
        "    for i in range(line_remote.shape[0]):\n",
        "        output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
        "        \n",
        "n_predictions = 3\n",
        "topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
        "topv\n",
        "\n",
        "predictions = []\n",
        "\n",
        "\n",
        "for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(-1.42) Irish\n",
            "(-1.65) French\n",
            "(-2.00) Czech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZnx7U_bLNUB",
        "colab": {}
      },
      "source": [
        "def predict(model, input_line, worker, n_predictions=3):\n",
        "    model = model.copy().get()\n",
        "    print('\\n> %s' % input_line)\n",
        "    model_remote = model.send(worker)\n",
        "    line_tensor = lineToTensor(input_line)\n",
        "    line_remote = line_tensor.copy().send(worker)\n",
        "    #line_tensor = lineToTensor(input_line)\n",
        "    #output = evaluate(model, line_remote)\n",
        "    # Get top N categories\n",
        "    hidden = model_remote.initHidden()\n",
        "    hidden_remote = hidden.copy().send(worker)\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for i in range(line_remote.shape[0]):\n",
        "            output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
        "        \n",
        "    topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(n_predictions):\n",
        "        value = topv[0][i].item()\n",
        "        category_index = topi[0][i].item()\n",
        "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "        predictions.append([value, all_categories[category_index]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3JBZLFbrLP3M",
        "outputId": "01a48078-4d0a-42a6-c995-b60ade59a429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "predict(model_pointers[\"alice\"], \"Qing\", alice)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Qing\n",
            "(-1.10) Chinese\n",
            "(-1.45) Korean\n",
            "(-2.72) German\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yhugkGySLRpe",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}