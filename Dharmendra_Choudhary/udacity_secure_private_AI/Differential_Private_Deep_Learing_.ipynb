{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential Private Deep Learing .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drc10723/udacity_secure_private_AI/blob/master/Differential_Private_Deep_Learing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsnI57x9qXlC",
        "colab_type": "text"
      },
      "source": [
        "# Training Deep Learning Model with Differntial Privacy\n",
        "\n",
        "If you have a private dataset and you want to train a deep learning model for some predictions, in most of cases you can't train model due to non-availabity of true labels. Let's take example, a hosptial has lots of unlabelled data for particular disease (like lung cancer). Even if other hospitals have labelled data, they can't share with others. In most of cases one hosptial doesn't have good number of labelled examples to train good model.\n",
        "\n",
        "\n",
        "## Problem Assumption:- \n",
        "\n",
        "*   N Hospitals ( Teachers) have some labelled data with same kind of labels\n",
        "*   One Hosptial ( Student) have some unlabelled data\n",
        "\n",
        "## Problem Solution :- \n",
        "\n",
        "\n",
        "*   Ask each of the N hospitals (Teachers) to train a model on their own datasets\n",
        "*   Use the N teachers models to predict on your local dataset, generating N labels for each datapoints\n",
        "*   Aggregate the N labels using a differential private (DP) query\n",
        "*   Train model with new aggregated labels on your own dataset\n",
        "\n",
        "\n",
        "\n",
        "Let's start by imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjlTH2x3ehtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe_bbNgmdn43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns1dKTcSoRZ4",
        "colab_type": "code",
        "outputId": "8a0cd05b-38e6-468f-d3e2-1940a02c7d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                      else \"cpu\")\n",
        "print(f\"Using {DEVICE} backend\")\n",
        "\n",
        "# number of teacher models.  \n",
        "# our student model accuracy will depend on this parameter\n",
        "num_teachers = 100 #@param {type:\"integer\"}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda backend\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XqEfI2DnNnu",
        "colab_type": "text"
      },
      "source": [
        "## Teacher Models Training\n",
        "\n",
        "We will use MNIST data as dummy data to train Teachers and Student Models.\n",
        "\n",
        "\n",
        "\n",
        "*   MNIST Training Data will be divided in N( equal to number of teachers) subsets and each subset will train one teacher model.\n",
        "*   MNIST Test Data will be used as private or student data and will be assumed unlabelled.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqcK-ZRIeuB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to tensor and normalize \n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([.5],[.5])])\n",
        "# load training data\n",
        "mnsit_dataset = datasets.MNIST('./mnsit', train=True, transform=train_transform, download=True, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tL_ggcdfmqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# divide mnist train data to num_teachers partitions\n",
        "total_size = len(mnsit_dataset)\n",
        "# length of each teacher dataset\n",
        "lengths = [int(total_size/num_teachers)]*num_teachers\n",
        "# list of all teacher dataset\n",
        "teacher_datasets = torch.utils.data.random_split(mnsit_dataset, lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgqoe8Adf64i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will create basic model, which will be used for teacher and student training both\n",
        "# It is not necessary to have same model structure for all teahders and even student model\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network,self).__init__()\n",
        "    # sequential layer : input size (batch_size, 28*28)\n",
        "    self.layer = nn.Sequential(nn.Linear(28*28, 256),\n",
        "                               # out size (batch_size, 256)\n",
        "                               nn.BatchNorm1d(256),\n",
        "                               # out size (batch_size, 256)\n",
        "                               nn.ReLU(),\n",
        "                               # out size (batch_size, 256)\n",
        "                               nn.Dropout(0.5),\n",
        "                               # out size (batch_size, 256)\n",
        "                               nn.Linear(256, 64),\n",
        "                               # out size (batch_size, 64)\n",
        "                               nn.BatchNorm1d(64),\n",
        "                               # out size (batch_size, 64)\n",
        "                               nn.ReLU(),\n",
        "                               # out size (batch_size, 64)\n",
        "                               nn.Dropout(0.5),\n",
        "                               # out size (batch_size, 64)\n",
        "                               nn.Linear(64, 10),\n",
        "                               # out size (batch_size, 10)\n",
        "                               # we will use logsoftmax instead softmax\n",
        "                               # softmax has expoential overflow issues\n",
        "                               nn.LogSoftmax(dim=1)\n",
        "                               # out size (batch_size, 10)\n",
        "                              )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x size : (batch_size, 1, 28, 28)\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    # x size : (batch_size, 784)\n",
        "    x = self.layer(x)\n",
        "    # x size : (batch_size, 10)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vDfitR6gAxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(dataset, checkpoint_file, num_epochs=10, do_validation=False):\n",
        "  \"\"\" \n",
        "  Train a model for given dataset for given number of epochs and\n",
        "  save last epoch model checkpoint\n",
        "  \n",
        "  Parameters: \n",
        "    dataset (torch.dataset): training data\n",
        "    checkpoint_file (str): filename for saving model\n",
        "    num_epochs (int): number of training epoch\n",
        "    do_validation (bool): perform validation by dividing dataset in 90:10 ratio\n",
        "          \n",
        "  Returns: None\n",
        "  \n",
        "  \"\"\"\n",
        "  # if validation divide dataset to train and test set 90:10 ratio\n",
        "  if do_validation:\n",
        "    dataset_size = len(dataset)\n",
        "    train_set, test_set = torch.utils.data.random_split(dataset, [int(0.9*dataset_size), int(0.1*dataset_size)])\n",
        "    # create train and test dataloader\n",
        "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(test_set, batch_size= 32, shuffle=True)\n",
        "  else:\n",
        "    # create train dataloader using full dataset\n",
        "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "  # create model and send to gpu\n",
        "  model = Network().to(DEVICE)\n",
        "  # we have used logsoftmax, so now NLLLoss\n",
        "  criterion = nn.NLLLoss()\n",
        "  # adam optimizer for training\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "  # train for num_epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # training accuracy and loss for logging\n",
        "    train_accuracy = 0\n",
        "    train_loss = 0\n",
        "    # training dataloader\n",
        "    for images, labels in trainloader:\n",
        "      # zero accumlated grads\n",
        "      optimizer.zero_grad()\n",
        "      # send images, labels to gpu\n",
        "      images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "      # run forward propagation\n",
        "      output = model.forward(images)\n",
        "      # calculate loss\n",
        "      loss = criterion(output, labels)\n",
        "      train_loss += loss.item()\n",
        "      # calculate accuracy \n",
        "      top_out, top_class = output.topk(1, dim=1)\n",
        "      success = (top_class==labels.view(*top_class.shape))\n",
        "      train_accuracy += success.sum().item()\n",
        "      # do backward propagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if do_validation:\n",
        "      # set model to evaluation\n",
        "      model.eval()\n",
        "      test_accuracy = 0\n",
        "      test_loss = 0\n",
        "      # do forward pass and calculate loss and accuracy \n",
        "      with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "          images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "          output = model.forward(images)\n",
        "          loss = criterion(output, labels)\n",
        "          test_loss += loss.item()\n",
        "          top_out, top_class = output.topk(1, dim=1) \n",
        "          success = (top_class==labels.view(*top_class.shape))\n",
        "          test_accuracy += success.sum().item()\n",
        "      # log train and test metrics\n",
        "      print(\"Epoch: {}\".format(epoch+1),\n",
        "            \"Train Loss: {:.3f}\".format(train_loss/len(trainloader)),\n",
        "            \"Train Accuracy: {:.3f}\".format(train_accuracy/len(train_set)),\n",
        "            \"Test Loss: {:.3f}\".format(test_loss/len(testloader)),\n",
        "            \"Test Accuracy: {:.3f}\".format(test_accuracy/len(test_set))\n",
        "           )\n",
        "      # set model to train\n",
        "      model.train()\n",
        "    else:\n",
        "      # log only training metrics if no validation\n",
        "      print(\"Epoch: {}\".format(epoch+1),\n",
        "            \"Train Loss: {:.3f}\".format(train_loss/len(trainloader)),\n",
        "            \"Train Accuracy: {:.3f}\".format(train_accuracy/len(dataset))\n",
        "           )\n",
        "    # save trained teacher model\n",
        "    torch.save(model.state_dict(), checkpoint_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLEIq8vZgZd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train all teachers models on MNIST partition datasets\n",
        "for teacher in range(num_teachers):\n",
        "  print(\"############################### Teacher {} Model Training #############################\".format(teacher+1))\n",
        "  train_model(teacher_datasets[teacher], f\"checkpoint_teacher_{teacher+1}.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv5oN__NobyE",
        "colab_type": "text"
      },
      "source": [
        "## Teacher Models Predictions\n",
        "\n",
        "Now we have trained N teachers models and we can share those trained models for student training.\n",
        "\n",
        "\n",
        "We have assumed MNIST test dataset, as student dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-HgSpTFggYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# student dataset transforms \n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([.5],[.5])])\n",
        "# load private student dataset\n",
        "private_dataset = datasets.MNIST('./mnsit', train=False, transform=test_transform, download=True)\n",
        "\n",
        "# mnist test dataset have 10000 examples\n",
        "private_data_size = len(private_dataset)\n",
        "\n",
        "# create dataloader for private train dataset\n",
        "private_dataloader = torch.utils.data.DataLoader(private_dataset, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFFD5ZKPr86_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_model(model_checkpoint, dataloader):\n",
        "  \"\"\" \n",
        "  Load a trained model and make predictions\n",
        "  \n",
        "  Parameters: \n",
        "    checkpoint_file (str): filename for trained model checkpoint\n",
        "    dataloader (DataLoader): dataloader instance\n",
        "          \n",
        "  Returns: \n",
        "    preds_list (torch.Tensor): predictions for whole dataset\n",
        "  \n",
        "  \"\"\"\n",
        "  # create model \n",
        "  model = Network()\n",
        "  # load model from checkpoint\n",
        "  state_dict = torch.load(model_checkpoint)\n",
        "  model.load_state_dict(state_dict)\n",
        "  # send model to gpu\n",
        "  model = model.to(DEVICE)\n",
        "  # list for batch predictions\n",
        "  preds_list = []\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "  # no gradients calculation needed\n",
        "  with torch.no_grad():\n",
        "    # iterate over dataset\n",
        "    for images, labels in dataloader:\n",
        "      images = images.to(DEVICE)\n",
        "      # calculate predictions ( log of predictions)\n",
        "      preds = model.forward(images)\n",
        "      # calculate top_class\n",
        "      top_preds, top_classes = preds.topk(k=1, dim=1)\n",
        "      # append batch top_classes tensor\n",
        "      preds_list.append(top_classes.view(-1))\n",
        "  # concat all batch predictions\n",
        "  preds_list = torch.cat(preds_list).cpu()\n",
        "  # return predictions\n",
        "  return preds_list "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehvTKsEPttUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7755d0a1-9a4c-4f87-c9f4-aaffc35baf64"
      },
      "source": [
        "# list of all teacher model predictions\n",
        "teacher_preds = []\n",
        "# predict for each teacher model\n",
        "for teacher in range(num_teachers):\n",
        "  teacher_preds.append(predict_model(f'checkpoint_teacher_{teacher+1}.pth', private_dataloader))\n",
        "# stack all teacher predictions\n",
        "teacher_preds = torch.stack(teacher_preds)\n",
        "print(teacher_preds.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDTfiE5N1cmW",
        "colab_type": "text"
      },
      "source": [
        "## Aggregating Teacher Predictions\n",
        "\n",
        "We have N predictions for each datapoint from our private dataset. We can aggregate N predictions using max query on bin counts for different labels.\n",
        "\n",
        "Can we train a model on those aggregated labels directly ? Yes, we can, but for increasing differenital privacy and keeping within some privacy budget, we will convert our aggreagte query to dp query. In dp query, we will add some amount of gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFpP_lPd_tnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epsilon budget for one aggregate dp query\n",
        "epsilon = 0.1 #@param {type:\"number\"}\n",
        "# number of labels\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWnNSbp5Q76",
        "colab_type": "text"
      },
      "source": [
        "we have assumed, student data is unlabelled. For analysis purpose we will use real labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4gsYby7rirD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# real targets, will not available for private dataset in real scenerio\n",
        "real_targets = private_dataset.targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHCbddtDss5A",
        "colab_type": "text"
      },
      "source": [
        "### Teacher Argmax Aggregation\n",
        "\n",
        "Aggregate N teacher predictions using max query on bin counts for different labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ByEkfPoyQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "809d917a-89c2-4a75-fcdc-4da9b4f09ca5"
      },
      "source": [
        "# teacher aggregation result\n",
        "teachers_argmax = list()\n",
        "for image_i in range(private_data_size):\n",
        "  # calculate bin count\n",
        "  label_counts = torch.bincount(teacher_preds[:, image_i], minlength=num_classes)\n",
        "  # take maximum bin count label\n",
        "  argmax_label = torch.argmax(label_counts)\n",
        "  teachers_argmax.append(argmax_label)\n",
        "# convert array to \n",
        "teachers_argmax = torch.tensor(teachers_argmax)\n",
        "# correct predictions\n",
        "argmax_correct = torch.sum(real_targets == teachers_argmax)\n",
        "print(\"Teachers argmax labels accuracy\", argmax_correct.item()/private_data_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Teachers argmax labels accuracy 0.9215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV_PkE7Ys0KX",
        "colab_type": "text"
      },
      "source": [
        "### Teacher Noisy Aggregation ( DP query)\n",
        "\n",
        "We use laplacian noise and beta will equal to **(sensitivity / epsilon )**.\n",
        "\n",
        "Sensitivity of argmax query will be one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_S-1lZ5o93S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45850183-9743-46a8-df9c-400d5c9d4b4b"
      },
      "source": [
        "# dp query results\n",
        "noisy_labels = list()\n",
        "for image_i in range(private_data_size):\n",
        "  # calculate bin count\n",
        "  label_counts = torch.bincount(teacher_preds[:, image_i], minlength=num_classes)\n",
        "  # calcuate beta for laplacian \n",
        "  beta = 1 / epsilon\n",
        "  \n",
        "  # add noise for each teacher predictions\n",
        "  for i in range(len(label_counts)):\n",
        "      label_counts[i] += np.random.laplace(0, beta, 1)[0]\n",
        "  # calculate dp label\n",
        "  noisy_label = torch.argmax(label_counts)\n",
        "  noisy_labels.append(noisy_label)\n",
        "\n",
        "noisy_labels = torch.tensor(noisy_labels)\n",
        "# accuracy for noisy or dp query results\n",
        "noisy_accuracy = torch.sum(real_targets == noisy_labels)\n",
        "\n",
        "print(\"Noisy label accuracy\", noisy_accuracy.item()/private_data_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noisy label accuracy 0.9155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzvbjQ__m5Ia",
        "colab_type": "text"
      },
      "source": [
        "## PATE Analysis\n",
        "\n",
        "**What is our epsilon budget, we have used ?** We will perform PATE analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K22A_cbm73A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ce833ae5-f705-416c-ed2e-6d982b7c857b"
      },
      "source": [
        "from syft.frameworks.torch.differential_privacy import pate"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 15:48:11.699311 139903046981504 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0717 15:48:11.724745 139903046981504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04IrN0KFAYRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory usage is getting pretty high with all predictions in PATE analysis,\n",
        "# using subset of predictions ( subset of mnist test dataset)\n",
        "# will help us understand importnace of private data size\n",
        "num_student_train = 2000 #@param {type:\"integer\"}\n",
        "teacher_preds1 = teacher_preds[:, :num_student_train].to(DEVICE)\n",
        "noisy_labels1 = noisy_labels[:num_student_train].to(DEVICE)\n",
        "teachers_argmax1 = teachers_argmax[:num_student_train].to(DEVICE)\n",
        "real_targets1 = real_targets[:num_student_train].to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amjcM56asCQC",
        "colab_type": "text"
      },
      "source": [
        "### Noisy Labels PATE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS2vWqN6nG9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7b89eea7-5a55-4c70-9130-5c9a34304196"
      },
      "source": [
        "# Data dependant and independant epsilon for noisy labels\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis_torch(preds=teacher_preds1, indices=noisy_labels1,\n",
        "                                                   noise_eps=epsilon, delta=1e-5, moments=10)\n",
        "print(f\"Data dependant epsilon {data_dep_eps.item()} data independent epsilon {data_ind_eps.item()}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/differential_privacy/pate.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(counts, dtype=torch.float) - torch.tensor(counts[winner], dtype=torch.float)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data dependant epsilon 17.959531784057617 data independent epsilon 91.51292419433594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NM0rcSKsRSC",
        "colab_type": "text"
      },
      "source": [
        "### Teacher Argmax PATE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZlOVKzwTlys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c1282206-f420-412a-8822-88ce2896ccf4"
      },
      "source": [
        "# Data dependant and independant epsilon for argmax labels\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis_torch(preds=teacher_preds1, indices=teachers_argmax1,\n",
        "                                                   noise_eps=epsilon, delta=1e-5, moments=10)\n",
        "print(f\"Data dependant epsilon {data_dep_eps.item()} data independent epsilon {data_ind_eps.item()}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/differential_privacy/pate.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(counts, dtype=torch.float) - torch.tensor(counts[winner], dtype=torch.float)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data dependant epsilon 17.83055305480957 data independent epsilon 91.51292419433594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NagLNxzssYLW",
        "colab_type": "text"
      },
      "source": [
        "### Real Labels PATE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPCPPiIgngxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "05333d46-7fa6-440c-8c85-5d0270635603"
      },
      "source": [
        "# Data dependant and independant epsilon for argmax labels\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis_torch(preds=teacher_preds1, indices=real_targets1,\n",
        "                                                   noise_eps=epsilon, delta=1e-5, moments=10)\n",
        "print(f\"Data dependant epsilon {data_dep_eps.item()} data independent epsilon {data_ind_eps.item()}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/differential_privacy/pate.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(counts, dtype=torch.float) - torch.tensor(counts[winner], dtype=torch.float)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data dependant epsilon 19.144325256347656 data independent epsilon 91.51292419433594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oDPSMdNs9pM",
        "colab_type": "text"
      },
      "source": [
        "## Student Model Training\n",
        "\n",
        "Differential privacy gaurantees that any amount of postprocessing can't increase epsilon value for given dataset, which means epsilon value will be less than or equal to PATE analysis values after training deep learning models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhPzW671nyL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save real labels\n",
        "private_real_labels = private_dataset.targets\n",
        "# replace real labels with noisy labels in private dataset\n",
        "private_dataset.targets = noisy_labels\n",
        "\n",
        "# create training and testing subset\n",
        "train_private_set = torch.utils.data.Subset(private_dataset, range(0, num_student_train))\n",
        "test_private_set = torch.utils.data.Subset(private_dataset, range(num_student_train, len(private_dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH55jgzNwXtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train student model with noisy labels\n",
        "student_model = train_model(train_private_set, f'checkpoint_student.pth', num_epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10zc5yEly8DL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fc99772-d0a7-4b3b-dd5d-7d46786e5e65"
      },
      "source": [
        "# create test loader\n",
        "private_testloader = torch.utils.data.DataLoader(test_private_set, batch_size=32)\n",
        "# get test predictions \n",
        "test_preds = predict_model(f'checkpoint_student.pth', private_testloader)\n",
        "# calculate test predictions \n",
        "correct = torch.sum(private_real_labels[num_student_train:] == test_preds)\n",
        "# accuracy\n",
        "print(f\"student model test accuracy {correct.item()/(len(private_dataset)-num_student_train)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "student model test accuracy 0.898375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egyl4l_xXqpW",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion \n",
        "\n",
        "As you can see, we are able to train a quite good accuracy model. \n",
        "\n",
        "Try different values of epsilon and number of teachers, you should able to observe following :- \n",
        "\n",
        "1.   More the numbers of teachers, less data dependent epsilon and more accuracy also\n",
        "2.   By adding noise, we are able to reduce privacy budget hugely ( See difference between data dependent and Independent epsilon)\n",
        "3.   Less the value of epsilon, more differntial privacy ( low data dependent and independent epsilon )\n",
        "4.   Given enough examples, deep learning model will able to remove noise added during DP query without reducing differential privacy.\n",
        "5.   More unlabelled student data, more accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQAx2WJiYv9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}