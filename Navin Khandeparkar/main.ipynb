{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mphhvsMWI4_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import random\n",
        "\n",
        "def load_datasets(test_sen=None):\n",
        "\n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "    LABEL = data.LabelField(dtype=torch.float)\n",
        "    train = data.TabularDataset(path='/content/query_classifier_data.csv', \n",
        "                        format='tsv', \n",
        "                        fields=[(\"question\",TEXT),\n",
        "                                (\"label\",LABEL)],  \n",
        "                        skip_header=True)\n",
        "    train_data, test_data = train.split(random_state=random.getstate())\n",
        "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
        "\n",
        "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.question), repeat=False, shuffle=True)\n",
        "\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jJGUVl-Nws8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (0, 1)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9jpnikZNeAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61c2583c-0824-4f99-91ed-9ae3fa43c037"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_datasets()\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.question[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.question[0]\n",
        "            if (text.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t\n",
        "\n",
        "learning_rate = 2e-5\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    \n",
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 40604\n",
            "Vector size of Text Vocabulary:  torch.Size([40604, 300])\n",
            "Label Length: 2\n",
            "Epoch: 1, Idx: 100, Training Loss: 0.0489, Training Accuracy:  100.00%\n",
            "Epoch: 1, Idx: 200, Training Loss: 0.2353, Training Accuracy:  93.75%\n",
            "Epoch: 1, Idx: 300, Training Loss: 0.1417, Training Accuracy:  96.88%\n",
            "Epoch: 1, Idx: 400, Training Loss: 0.1391, Training Accuracy:  96.88%\n",
            "Epoch: 1, Idx: 500, Training Loss: 0.1392, Training Accuracy:  96.88%\n",
            "Epoch: 1, Idx: 600, Training Loss: 0.0397, Training Accuracy:  100.00%\n",
            "Epoch: 1, Idx: 700, Training Loss: 0.0523, Training Accuracy:  100.00%\n",
            "Epoch: 1, Idx: 800, Training Loss: 0.1402, Training Accuracy:  96.88%\n",
            "Epoch: 01, Train Loss: 0.146, Train Acc: 96.62%, Val. Loss: 0.133421, Val. Acc: 96.44%\n",
            "Epoch: 2, Idx: 100, Training Loss: 0.0290, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 200, Training Loss: 0.1417, Training Accuracy:  96.88%\n",
            "Epoch: 2, Idx: 300, Training Loss: 0.1393, Training Accuracy:  96.88%\n",
            "Epoch: 2, Idx: 400, Training Loss: 0.0343, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 500, Training Loss: 0.1395, Training Accuracy:  96.88%\n",
            "Epoch: 2, Idx: 600, Training Loss: 0.0305, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 700, Training Loss: 0.0484, Training Accuracy:  100.00%\n",
            "Epoch: 2, Idx: 800, Training Loss: 0.2406, Training Accuracy:  93.75%\n",
            "Epoch: 02, Train Loss: 0.142, Train Acc: 96.73%, Val. Loss: 0.132237, Val. Acc: 96.44%\n",
            "Epoch: 3, Idx: 100, Training Loss: 0.1392, Training Accuracy:  96.88%\n",
            "Epoch: 3, Idx: 200, Training Loss: 0.3725, Training Accuracy:  90.62%\n",
            "Epoch: 3, Idx: 300, Training Loss: 0.0223, Training Accuracy:  100.00%\n",
            "Epoch: 3, Idx: 400, Training Loss: 0.1406, Training Accuracy:  96.88%\n",
            "Epoch: 3, Idx: 500, Training Loss: 0.0346, Training Accuracy:  100.00%\n",
            "Epoch: 3, Idx: 600, Training Loss: 0.2466, Training Accuracy:  93.75%\n",
            "Epoch: 3, Idx: 700, Training Loss: 0.2514, Training Accuracy:  93.75%\n",
            "Epoch: 3, Idx: 800, Training Loss: 0.0351, Training Accuracy:  100.00%\n",
            "Epoch: 03, Train Loss: 0.142, Train Acc: 96.74%, Val. Loss: 0.132164, Val. Acc: 96.44%\n",
            "Epoch: 4, Idx: 100, Training Loss: 0.2445, Training Accuracy:  93.75%\n",
            "Epoch: 4, Idx: 200, Training Loss: 0.1391, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 300, Training Loss: 0.1394, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 400, Training Loss: 0.0267, Training Accuracy:  100.00%\n",
            "Epoch: 4, Idx: 500, Training Loss: 0.0314, Training Accuracy:  100.00%\n",
            "Epoch: 4, Idx: 600, Training Loss: 0.1391, Training Accuracy:  96.88%\n",
            "Epoch: 4, Idx: 700, Training Loss: 0.2461, Training Accuracy:  93.75%\n",
            "Epoch: 4, Idx: 800, Training Loss: 0.0165, Training Accuracy:  100.00%\n",
            "Epoch: 04, Train Loss: 0.146, Train Acc: 96.42%, Val. Loss: 0.131315, Val. Acc: 96.44%\n",
            "Epoch: 5, Idx: 100, Training Loss: 0.0133, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 200, Training Loss: 0.0164, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 300, Training Loss: 0.1326, Training Accuracy:  96.88%\n",
            "Epoch: 5, Idx: 400, Training Loss: 0.0105, Training Accuracy:  100.00%\n",
            "Epoch: 5, Idx: 500, Training Loss: 0.1342, Training Accuracy:  96.88%\n",
            "Epoch: 5, Idx: 600, Training Loss: 0.1306, Training Accuracy:  96.88%\n",
            "Epoch: 5, Idx: 700, Training Loss: 0.0582, Training Accuracy:  96.88%\n",
            "Epoch: 5, Idx: 800, Training Loss: 0.0300, Training Accuracy:  96.88%\n",
            "Epoch: 05, Train Loss: 0.088, Train Acc: 97.67%, Val. Loss: 0.049263, Val. Acc: 98.26%\n",
            "Epoch: 6, Idx: 100, Training Loss: 0.1417, Training Accuracy:  93.75%\n",
            "Epoch: 6, Idx: 200, Training Loss: 0.0074, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 300, Training Loss: 0.0184, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 400, Training Loss: 0.0827, Training Accuracy:  96.88%\n",
            "Epoch: 6, Idx: 500, Training Loss: 0.0236, Training Accuracy:  96.88%\n",
            "Epoch: 6, Idx: 600, Training Loss: 0.0280, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 700, Training Loss: 0.0143, Training Accuracy:  100.00%\n",
            "Epoch: 6, Idx: 800, Training Loss: 0.0162, Training Accuracy:  100.00%\n",
            "Epoch: 06, Train Loss: 0.046, Train Acc: 98.11%, Val. Loss: 0.036710, Val. Acc: 98.29%\n",
            "Epoch: 7, Idx: 100, Training Loss: 0.0067, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 200, Training Loss: 0.0236, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 300, Training Loss: 0.0018, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 400, Training Loss: 0.0041, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 500, Training Loss: 0.0029, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 600, Training Loss: 0.0976, Training Accuracy:  96.88%\n",
            "Epoch: 7, Idx: 700, Training Loss: 0.0068, Training Accuracy:  100.00%\n",
            "Epoch: 7, Idx: 800, Training Loss: 0.0361, Training Accuracy:  96.88%\n",
            "Epoch: 07, Train Loss: 0.023, Train Acc: 99.26%, Val. Loss: 0.111182, Val. Acc: 96.44%\n",
            "Epoch: 8, Idx: 100, Training Loss: 0.0031, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 200, Training Loss: 0.0016, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 300, Training Loss: 0.0153, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 400, Training Loss: 0.0021, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 500, Training Loss: 0.0031, Training Accuracy:  100.00%\n",
            "Epoch: 8, Idx: 600, Training Loss: 0.0746, Training Accuracy:  96.88%\n",
            "Epoch: 8, Idx: 700, Training Loss: 0.0622, Training Accuracy:  96.88%\n",
            "Epoch: 8, Idx: 800, Training Loss: 0.0014, Training Accuracy:  100.00%\n",
            "Epoch: 08, Train Loss: 0.016, Train Acc: 99.44%, Val. Loss: 0.016817, Val. Acc: 99.27%\n",
            "Epoch: 9, Idx: 100, Training Loss: 0.0020, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 200, Training Loss: 0.0020, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 300, Training Loss: 0.0010, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 400, Training Loss: 0.0007, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 500, Training Loss: 0.0226, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 600, Training Loss: 0.0001, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 700, Training Loss: 0.0004, Training Accuracy:  100.00%\n",
            "Epoch: 9, Idx: 800, Training Loss: 0.0006, Training Accuracy:  100.00%\n",
            "Epoch: 09, Train Loss: 0.013, Train Acc: 99.51%, Val. Loss: 0.012350, Val. Acc: 99.34%\n",
            "Epoch: 10, Idx: 100, Training Loss: 0.0011, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 200, Training Loss: 0.0021, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 300, Training Loss: 0.0451, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 400, Training Loss: 0.0123, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 500, Training Loss: 0.0709, Training Accuracy:  93.75%\n",
            "Epoch: 10, Idx: 600, Training Loss: 0.0048, Training Accuracy:  100.00%\n",
            "Epoch: 10, Idx: 700, Training Loss: 0.0922, Training Accuracy:  93.75%\n",
            "Epoch: 10, Idx: 800, Training Loss: 0.0009, Training Accuracy:  100.00%\n",
            "Epoch: 10, Train Loss: 0.012, Train Acc: 99.51%, Val. Loss: 0.014716, Val. Acc: 99.29%\n",
            "Test Loss: 0.014, Test Acc: 99.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx2j9kX3kOhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc1cf665-b1ee-4097-8c24-96581343b9f8"
      },
      "source": [
        "#@title Install Faiss, TF 2.0, and our Github. Double Click to see code\n",
        "\n",
        "#To use CPU FAISS use\n",
        "!wget  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "#To use GPU FAISS use\n",
        "# !wget  https://anaconda.org/pytorch/faiss-gpu/1.2.1/download/linux-64/faiss-gpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "!tar xvjf faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "!cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/\n",
        "!pip install mkl\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "!pip install https://github.com/re-search/DocProduct/archive/v0.2.0_dev.zip\n",
        "!pip install gpt2-estimator\n",
        "!pip install pyarrow\n",
        "\n",
        "#@title Downaload all model checkpoints, and question/answer data. Double click to see this code\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "import os\n",
        "import requests\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Download the file from `url` and save it locally under `file_name`:\n",
        "urllib.request.urlretrieve('https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_v1.0_pubmed_pmc.tar.gz', 'BioBert.tar.gz')\n",
        "\n",
        "if not os.path.exists('BioBertFolder'):\n",
        "    os.makedirs('BioBertFolder')\n",
        "    \n",
        "import tarfile\n",
        "tar = tarfile.open(\"BioBert.tar.gz\")\n",
        "tar.extractall(path='BioBertFolder/')\n",
        "tar.close()\n",
        "\n",
        "file_id = '1uCXv6mQkFfpw5txGnVCsl93Db7t5Z2mp'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'Float16EmbeddingsExpanded5-27-19.pkl')\n",
        "\n",
        "file_id = 'https://onedrive.live.com/download?cid=9DEDF3C1E2D7E77F&resid=9DEDF3C1E2D7E77F%2132792&authkey=AEQ8GtkcDbe3K98'\n",
        "    \n",
        "urllib.request.urlretrieve( file_id, 'DataAndCheckpoint.zip')\n",
        "\n",
        "if not os.path.exists('newFolder'):\n",
        "    os.makedirs('newFolder')\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('DataAndCheckpoint.zip', 'r')\n",
        "zip_ref.extractall('newFolder')\n",
        "zip_ref.close()\n",
        "\n",
        "#@title Load model weights and Q&A data. Double click to see code\n",
        "\n",
        "from docproduct.predictor import RetreiveQADoc\n",
        "\n",
        "pretrained_path = 'BioBertFolder/biobert_v1.0_pubmed_pmc/'\n",
        "# ffn_weight_file = None\n",
        "bert_ffn_weight_file = 'newFolder/models/bertffn_crossentropy/bertffn'\n",
        "embedding_file = 'Float16EmbeddingsExpanded5-27-19.pkl'\n",
        "\n",
        "doc = RetreiveQADoc(pretrained_path=pretrained_path,\n",
        "ffn_weight_file=None,\n",
        "bert_ffn_weight_file=bert_ffn_weight_file,\n",
        "embedding_file=embedding_file)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-19 17:36:04--  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
            "Resolving anaconda.org (anaconda.org)... 104.17.92.24, 104.17.93.24, 2606:4700::6811:5d18, ...\n",
            "Connecting to anaconda.org (anaconda.org)|104.17.92.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://binstar-cio-packages-prod.s3.amazonaws.com/5a15c9c5c376961204909d87/5aa7f0a65571b411e5c259be?response-content-disposition=attachment%3B%20filename%3D%22faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=60&X-Amz-Date=20190819T173604Z&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEPf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBZvEzRSGR7YLDmfcwS6Z4Lihpg%2Bb9zlOYoCN9dlVJj0AiAJBSDnjSbmrBLYiLg%2BPio68b1vB01644vUHiU9r5TiMyrjAwiQ%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDQ1NTg2NDA5ODM3OCIMIltCWY4aw4FOff93KrcDoXG9v%2BX4Eq2FEFkuv1EYQ%2BDGwYrEgagVhmsnhW0wNOzuYSX3Rsgz5WpAZRW1r%2B4o%2BQUmBFxoHiOxoRo73t35EZlT2RTXA41u%2Bq675CdmttjHksPIluGP4UAcebawb3%2BDSg6Gc3QzCzkJR3eYSbbhS%2Bp2SQ5LIWL0%2FeRgaisff2E7GjOUwq%2B6CIGuceKf94OEGTuy47oY%2BoDiHjk8MPnOD712uNN6YGGsHsxyq%2BX00Kr75jg3Yg4aCZs6uBsgj2XOg1%2F5OVRpandoLbeM5%2BouWUs76aKyd19fg2a%2BBvTV8CluaXK8TPzGVce5yKIhDFOy2h4DxWfa6ed1UGQgSSb96Ptq2PMd%2F7bHgCRAxgD9r4QAY9OkHm5J0n3hjsjVaZfFY%2B9xsrnBa1ddSc57BnqwrumM3HqUYfPszSu7d%2Ftnjw5PspOtWsq6UdpSPxzPbJM8S5WUQTZ43swuDxrqaFmStX7aG21wvO0EMOPXDk1B1Q06BsPdCt70I3FS6QmgdhJwoMHX%2BbMr8YtmLH0dt8u5LceKv8%2BI3pWLaR6mxyQZpKKBBcbXu0omFE9yp3Cd6z8rzR7ZxayWyTC35OrqBTq1Ae5ZQf6lpIoi7ZDBm5ojIFuxwmTUZibhmbu5YNT8JVP6Ce0TvqCKt1K2RwGJ3uxR8SzNgmdbCEtQxnesCJFS3FEorqB5lE2z%2FouI9YsSPJ6mQfuoPnikzXq8O9T%2F%2BhvAIG1%2BRCGD5P7P6CA22yAExvfl%2Fzf%2B0clLyhAYg3hvvOrgSCQEASSEJVG%2FXTIsXFf5TZbdILMlk6yrSrOrh1pWi1LCk5s7Cdsg%2F8ZiRMxIut0NV3vuwMc%3D&X-Amz-Credential=ASIAWUI46DZFJGY5PUHE%2F20190819%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=5e72d2ed897dafe325a930d66654c85825aa5d71ba9824ea16f106378ec82dea [following]\n",
            "--2019-08-19 17:36:04--  https://binstar-cio-packages-prod.s3.amazonaws.com/5a15c9c5c376961204909d87/5aa7f0a65571b411e5c259be?response-content-disposition=attachment%3B%20filename%3D%22faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=60&X-Amz-Date=20190819T173604Z&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEPf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBZvEzRSGR7YLDmfcwS6Z4Lihpg%2Bb9zlOYoCN9dlVJj0AiAJBSDnjSbmrBLYiLg%2BPio68b1vB01644vUHiU9r5TiMyrjAwiQ%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDQ1NTg2NDA5ODM3OCIMIltCWY4aw4FOff93KrcDoXG9v%2BX4Eq2FEFkuv1EYQ%2BDGwYrEgagVhmsnhW0wNOzuYSX3Rsgz5WpAZRW1r%2B4o%2BQUmBFxoHiOxoRo73t35EZlT2RTXA41u%2Bq675CdmttjHksPIluGP4UAcebawb3%2BDSg6Gc3QzCzkJR3eYSbbhS%2Bp2SQ5LIWL0%2FeRgaisff2E7GjOUwq%2B6CIGuceKf94OEGTuy47oY%2BoDiHjk8MPnOD712uNN6YGGsHsxyq%2BX00Kr75jg3Yg4aCZs6uBsgj2XOg1%2F5OVRpandoLbeM5%2BouWUs76aKyd19fg2a%2BBvTV8CluaXK8TPzGVce5yKIhDFOy2h4DxWfa6ed1UGQgSSb96Ptq2PMd%2F7bHgCRAxgD9r4QAY9OkHm5J0n3hjsjVaZfFY%2B9xsrnBa1ddSc57BnqwrumM3HqUYfPszSu7d%2Ftnjw5PspOtWsq6UdpSPxzPbJM8S5WUQTZ43swuDxrqaFmStX7aG21wvO0EMOPXDk1B1Q06BsPdCt70I3FS6QmgdhJwoMHX%2BbMr8YtmLH0dt8u5LceKv8%2BI3pWLaR6mxyQZpKKBBcbXu0omFE9yp3Cd6z8rzR7ZxayWyTC35OrqBTq1Ae5ZQf6lpIoi7ZDBm5ojIFuxwmTUZibhmbu5YNT8JVP6Ce0TvqCKt1K2RwGJ3uxR8SzNgmdbCEtQxnesCJFS3FEorqB5lE2z%2FouI9YsSPJ6mQfuoPnikzXq8O9T%2F%2BhvAIG1%2BRCGD5P7P6CA22yAExvfl%2Fzf%2B0clLyhAYg3hvvOrgSCQEASSEJVG%2FXTIsXFf5TZbdILMlk6yrSrOrh1pWi1LCk5s7Cdsg%2F8ZiRMxIut0NV3vuwMc%3D&X-Amz-Credential=ASIAWUI46DZFJGY5PUHE%2F20190819%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=5e72d2ed897dafe325a930d66654c85825aa5d71ba9824ea16f106378ec82dea\n",
            "Resolving binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)... 52.216.86.179\n",
            "Connecting to binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)|52.216.86.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4106816 (3.9M) [application/x-tar]\n",
            "Saving to: ‘faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2’\n",
            "\n",
            "\r          faiss-cpu   0%[                    ]       0  --.-KB/s               \rfaiss-cpu-1.2.1-py3 100%[===================>]   3.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-08-19 17:36:05 (37.6 MB/s) - ‘faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2’ saved [4106816/4106816]\n",
            "\n",
            "info/hash_input.json\n",
            "info/has_prefix\n",
            "info/index.json\n",
            "info/git\n",
            "info/files\n",
            "info/LICENSE.txt\n",
            "info/about.json\n",
            "info/paths.json\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/dependency_links.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/not-zip-safe\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/requires.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/top_level.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/native_libs.txt\n",
            "info/test/run_test.py\n",
            "info/test/run_test.sh\n",
            "info/test/tests/run_tests.sh\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/SOURCES.txt\n",
            "info/recipe/conda_build_config.yaml\n",
            "info/recipe/build.sh\n",
            "info/test/tests/CMakeLists.txt\n",
            "info/test/tests/Makefile\n",
            "info/recipe/meta.yaml.template\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/PKG-INFO\n",
            "info/test/tests/test_factory.py\n",
            "info/test/tests/test_ivfpq_codec.cpp\n",
            "info/recipe/meta.yaml\n",
            "info/recipe/setup.py\n",
            "info/test/tests/test_blas.cpp\n",
            "info/recipe/makefile.inc\n",
            "info/test/tests/test_ivfpq_indexing.cpp\n",
            "info/test/tests/test_ondisk_ivf.cpp\n",
            "info/test/tests/test_build_blocks.py\n",
            "info/test/tests/test_merge.cpp\n",
            "info/test/tests/test_pairs_decoding.cpp\n",
            "info/test/tests/test_index_composite.py\n",
            "lib/python3.6/site-packages/faiss/__init__.py\n",
            "lib/python3.6/site-packages/faiss/__pycache__/__init__.cpython-36.pyc\n",
            "info/test/tests/test_index.py\n",
            "info/test/tests/test_blas\n",
            "lib/python3.6/site-packages/faiss/__pycache__/swigfaiss.cpython-36.pyc\n",
            "lib/python3.6/site-packages/faiss/swigfaiss.py\n",
            "lib/python3.6/site-packages/faiss/_swigfaiss.so\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.6/dist-packages (2019.0)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (from mkl) (2019.0)\n",
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 78kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.4)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 37.9MB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n",
            "Collecting https://github.com/re-search/DocProduct/archive/v0.2.0_dev.zip\n",
            "\u001b[?25l  Downloading https://github.com/re-search/DocProduct/archive/v0.2.0_dev.zip\n",
            "\u001b[K     / 18.3MB 6.7MB/s\n",
            "\u001b[?25hCollecting pycurl (from docproduct==0.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/b3/0f3979633b7890bab6098d84c84467030b807a1e2b31f5d30103af5a71ca/pycurl-7.43.0.3.tar.gz (215kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (0.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (3.0.3)\n",
            "Collecting tensorflow==2.0.0-alpha0 (from docproduct==0.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (2.0.0a0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (2.2.4)\n",
            "Collecting keras-pos-embd==0.9.0 (from docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/56/5e/7b1e933104a25f2039b6788e392a650671e3bcbee6404ea29dcb92295614/keras-pos-embd-0.9.0.tar.gz\n",
            "Collecting keras-transformer==0.21.0 (from docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/3a/ad25f5c71adc6b8aa73f71b1367be873b4103125a614ba57c006d1a9b1ff/keras-transformer-0.21.0.tar.gz\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (4.28.1)\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (0.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from docproduct==0.2.0) (1.12.0)\n",
            "Collecting argparse (from docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->docproduct==0.2.0) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->docproduct==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->docproduct==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->docproduct==0.2.0) (2.4.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.14.0a20190301)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.33.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->docproduct==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->docproduct==0.2.0) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->docproduct==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->docproduct==0.2.0) (3.13)\n",
            "Collecting keras-multi-head==0.18.0 (from keras-transformer==0.21.0->docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/5d/8156def9ca75c55bb87819618e9a3e1f8e587c722570e2e93ad616b9269d/keras-multi-head-0.18.0.tar.gz\n",
            "Collecting keras-layer-normalization==0.11.0 (from keras-transformer==0.21.0->docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/b0/c786d5a5e79d985281a06da0a1f3f559cf425921464e6b07b9f1cb093a5a/keras-layer-normalization-0.11.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward==0.4.0 (from keras-transformer==0.21.0->docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/91/21/4eefba0b6ea01de9c6e469970a39dbdbce14e5183a20274d9a181f55eaa8/keras-position-wise-feed-forward-0.4.0.tar.gz\n",
            "Collecting keras-embed-sim==0.3.0 (from keras-transformer==0.21.0->docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/16/b05954f9578ded225fd1bd56154ade949782c03b668a1fc424d5050e868a/keras-embed-sim-0.3.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->docproduct==0.2.0) (0.21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->docproduct==0.2.0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0->docproduct==0.2.0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0->docproduct==0.2.0) (3.1.1)\n",
            "Collecting keras-self-attention==0.39.0 (from keras-multi-head==0.18.0->keras-transformer==0.21.0->docproduct==0.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/91/70/51150779d5bbd1488a30c62026b141073873faf81eac7a62c6460cb5efe0/keras-self-attention-0.39.0.tar.gz\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->docproduct==0.2.0) (0.13.2)\n",
            "Building wheels for collected packages: docproduct, pycurl, keras-pos-embd, keras-transformer, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for docproduct (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docproduct: filename=docproduct-0.2.0-cp36-none-any.whl size=61651 sha256=bc3cc50d48445dbb9adba453b1075e19887fa2e55fa86e1514aab3de365dee4c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v8_irjp9/wheels/d6/ff/7e/7c265da92fd0d24132f65f8f7e28c63c8381f5478c32bd9135\n",
            "  Building wheel for pycurl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycurl: filename=pycurl-7.43.0.3-cp36-cp36m-linux_x86_64.whl size=282656 sha256=e63da3ca31b0bdae7a7a50fe01bb94099039d7babe049a8fc13e99a5be9a6ac4\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/40/f5/7b4f2285aca871b5173887a6c69127210d92806c0d3a977e51\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.9.0-cp36-none-any.whl size=6902 sha256=77d5bd300cb797583adb8d5099f5b6f5f9eda8f3268dd71ecd1e3df627563593\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/97/65/170068ed0a4bd2185d561afee6c93e23e87e8d735d61389590\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.21.0-cp36-none-any.whl size=10101 sha256=2635d5585fdb73f77382125129ae99b2e594fa67b084a1d42e7f92b3322de4ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/9f/ff/3b38f44f6db035cfd33cff4909edcc4864a6aeec80d9deaf23\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.18.0-cp36-none-any.whl size=14986 sha256=4326bc82d3092543979c6991033eb11b38fe5d3b3b2194906f49f5292b596e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/88/83/d7680876b48974c3c11fc334ed1d0a480ae218764062385bf3\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.11.0-cp36-none-any.whl size=4646 sha256=867b397cf3e1f9071307cf03f077f19e041c033eee28ae23fe3946ab934108f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/dc/2e/3ac54a6b948bff68cb999d210c6ebf9e22df7a4a24cf114436\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.4.0-cp36-none-any.whl size=4654 sha256=09a7ab7f477e5ac28423cb6524ab84708530506549ed81727d6f7246856ed9ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/a1/13/3c913efde102d56ac584f61004a9fec6f8859b6feec6aa7aa7\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.3.0-cp36-none-any.whl size=3530 sha256=e9928a07562a35ef2cc2b92af0c8b4d6ef4a93c24f8eb82fddca57be2ec3eb49\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/f2/c6/0610efe9730c708b24ec29c25cebd38eb485acbc2eee7b5634\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.39.0-cp36-none-any.whl size=17956 sha256=d637968838ad7d5cfc46fd0b1c8a93d645aabecb7cbb72fc58e53812390b3007\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/d6/3e/cac34bf035198e38947006910f3ecb25613d6d9d76ea6d8ef2\n",
            "Successfully built docproduct pycurl keras-pos-embd keras-transformer keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: pycurl, tensorflow, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, argparse, docproduct\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed argparse-1.4.0 docproduct-0.2.0 keras-embed-sim-0.3.0 keras-layer-normalization-0.11.0 keras-multi-head-0.18.0 keras-pos-embd-0.9.0 keras-position-wise-feed-forward-0.4.0 keras-self-attention-0.39.0 keras-transformer-0.21.0 pycurl-7.43.0.3 tensorflow-2.0.0a0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt2-estimator\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/78/a83c8f020c7356bb592260fb92f13b5bf6405a31ffe7580f4847436d970a/gpt2_estimator-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (2.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (0.24.2)\n",
            "Collecting regex (from gpt2-estimator)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0a0 in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (2.0.0a0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from gpt2-estimator) (3.7.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt2-estimator) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt2-estimator) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt2-estimator) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt2-estimator) (2019.6.16)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->gpt2-estimator) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->gpt2-estimator) (2018.9)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.0.8)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.14.0a20190301)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.33.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->gpt2-estimator) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0a0->gpt2-estimator) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0a0->gpt2-estimator) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0a0->gpt2-estimator) (0.15.5)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609254 sha256=5da92493756294eeb32d000b763442788ee639dfc4850fb74be42b6391d2f356\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, gpt2-estimator\n",
            "Successfully installed gpt2-estimator-0.1.0 regex-2019.8.19\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.16.4)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Failed to load GPU Faiss: No module named 'faiss.swigfaiss_gpu'\n",
            "Faiss falling back to CPU-only.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w14-NfIRNicQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6bec202e-d15c-4659-adea-c5b5c278d3f9"
      },
      "source": [
        "''' predict on a single sentence just for the testing purpose. '''\n",
        "test_sen1 = [0,0]\n",
        "test_sen1[0] = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "test_sen1[1] = \"I have fever for past few days.\"\n",
        "\n",
        "for t in test_sen1:\n",
        "  test_sen1 = TEXT.preprocess(t)\n",
        "  test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "\n",
        "  test_sen = np.asarray(test_sen1)\n",
        "  test_sen = torch.LongTensor(test_sen)\n",
        "  test_tensor = Variable(test_sen, volatile=True)\n",
        "  test_tensor = test_tensor.cuda()\n",
        "  model.eval()\n",
        "  output = model(test_tensor, 1)\n",
        "  out = F.softmax(output, 1)\n",
        "\n",
        "  if (torch.argmax(out[0]) == 1):\n",
        "      print (\"Not Health Related\")\n",
        "  else:\n",
        "      print (\"Health Related\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not Health Related\n",
            "Health Related\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNnMGYst_7IN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d67c94da-2fa0-4a53-ece8-fdb079f12027"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoYWnM_Jkgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "cellView": "form",
        "outputId": "5de8f3ed-8e48-4fa8-cf95-b1bad1febc6c"
      },
      "source": [
        "your_query    = \"I have fever for past few days.\"        #@param {type:\"string\"}\n",
        "\n",
        "test_sen1 = TEXT.preprocess(your_query)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "test_sen = np.asarray(test_sen1)\n",
        "test_sen = torch.LongTensor(test_sen)\n",
        "test_tensor = Variable(test_sen, volatile=True)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor, 1)\n",
        "out = F.softmax(output, 1)\n",
        "\n",
        "if (torch.argmax(out[0]) == 1):\n",
        "    print (\"\\nNot Health Related\\n\")\n",
        "else:\n",
        "    print (\"\\nHealth Related\\n\")\n",
        "    # if Health Related then\n",
        "    \n",
        "    from nltk.corpus import stopwords \n",
        "    from nltk.tokenize import word_tokenize\n",
        "      \n",
        "    stop_words = set(stopwords.words('english')) \n",
        "\n",
        "    word_tokens = word_tokenize(your_query) \n",
        "\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "\n",
        "    filtered_sentence = [] \n",
        "\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)    \n",
        "#     print(filtered_sentence)\n",
        "    pos = nltk.pos_tag(filtered_sentence)\n",
        "#     print(pos)\n",
        "   \n",
        "    required_keywords = [\"sypmtom\", \"duration\", \"medicine\"]\n",
        "    \n",
        "    \n",
        "    \n",
        "    returned_results = doc.predict( your_query ,\n",
        "                  search_by='answer', topk=5, answer_only=True)\n",
        "    print('')\n",
        "    for jk in range(len(returned_results)):\n",
        "        if (len(returned_results[jk])>50):\n",
        "#           tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "          print ('\\n-----\\n'.join(tokenizer.tokenize(returned_results[jk])))\n",
        "          print('')\n",
        "          break"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  import sys\n",
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Health Related\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1it [00:00,  3.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "hi that would be symptoms of inflammation in the body and your immune system is trying to fight it.\n",
            "-----\n",
            "you may or may not develop fever… i guess things will show up in the next few hours whether you will have any other symptoms or not.\n",
            "-----\n",
            "take analgesics only and only if necessary … let me know here if you developed any other symptom\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zi4fuam_yXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}