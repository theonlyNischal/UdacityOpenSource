{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sixty AI - Training",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/60-days-of-udacity-sixty-ai/blob/master/03%20Sixty%20AI%20Training/Sixty_AI_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Sixty AI - Train a GPT-2 on Text Corpus  \n",
        "\n",
        "<br/> We are now going to use our processed text corpus to train OpenAI's GPT-2 Model. \n",
        "<br/> Of course, we are not going to use the _original_ GPT-2 from OpenAI. We are instead going to use _GPT-2 Simple_ from _**Max Woolf**_.\n",
        "<br/> GPT-2 Simple is a pretrained GPT-2 model from OpenAI, but with the added functionality of _Finetuning_. We will use our tiny text corpus to finetune a full blown GPT-2 (small _117M_ model) , so that it starts generating some creative text content on its own. This is _NLP Transfer Learning_ live in action! \n",
        "\n",
        "<br/> For more information about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the GPT-2 Simple library \n",
        "!pip install -q gpt-2-simple\n",
        "\n",
        "# Import Export business \n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory now uses an Nvidia T4 GPU, which is slightly faster than the old Nvidia K80 GPU for training GPT-2, and has more memory allowing us to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "Let's verify which GPU is active."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "7cf845c6-55e7-4378-8f06-f860dae270c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug  1 19:17:39 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    29W /  70W |   6852MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "We need to download the GPT-2 model first. \n",
        "\n",
        "There are two sizes of GPT-2:\n",
        "\n",
        "* `117M` (default): the \"small\" model, 500MB on disk.\n",
        "* `345M`: the \"medium\" model, 1.5GB on disk.\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. \n",
        "<br/>We will use the smaller 117M model to start things off. \n",
        "<br/>The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "9ccd02f0-4e1a-41ab-e1c7-89cb3a7023c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Download the smaller 117M model \n",
        "gpt2.download_gpt2(model_name=\"117M\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 267Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 93.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 845Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 119Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 257Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 154Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 172Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "<br/>In case you're a little skeptical about putting down the Google Drive auth code (you should be!), I suggest you check out what is going on under the hood in the `gpt-2-simple` library. Simply [click this link](https://github.com/minimaxir/gpt-2-simple/blob/master/gpt_2_simple/gpt_2.py \"click this link\"). \n",
        "\n",
        "<br/>TL;DR: _It's Safe!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "748f08ad-1e90-4981-aa35-cd6568041846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading our Text Corpus to be Trained \n",
        "\n",
        "Let's upload our text corpus in the _'Files'_ section (this has to be done manually). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"final_text_corpus.csv\"\n",
        "model_name = 'run1' # Default Name "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "Finally, it's time to _finetune_ our Simple GPT-2 Model on our extracted corpus of text. \n",
        "<br/>The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (setting `steps = -1` will run the finetuning indefinitely)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps and when the cell is stopped.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "79acffb5-6db9-4bd2-de3f-f07c0e3804ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "Parameters for gpt2.finetune(): \n",
        "\n",
        "restore_from: Set to 'fresh' to start training from the base GPT-2, \n",
        "  or set to 'latest' to restart training from an existing checkpoint.\n",
        "\n",
        "sample_every: Number of steps to print example output\n",
        "\n",
        "print_every: Number of steps to print training progress.\n",
        "\n",
        "learning_rate:  Learning rate for the training. \n",
        "  (default '1e-4', can lower to '1e-5' if you have <1MB input data)\n",
        "\n",
        "run_name: subfolder within 'checkpoint' to save the model. \n",
        "  This is useful if you want to work with multiple models \n",
        "  (will also need to specify  'run_name' when loading the model)\n",
        "\n",
        "overwrite: Set to 'True' if you want to continue finetuning an existing \n",
        "  model (w/ restore_from='latest') without creating duplicate copies. \n",
        "\"\"\"\n",
        "\n",
        "# Start the tf session \n",
        "# LOL.....why they have a 'session' in tf?! :D \n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "# We will train for 1000 epochs, as I have noticed that beyond 1000\n",
        "# epochs the model starts to overfit on the data\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='117M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name=model_name,\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 16:33:05.579495 140450407880576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0801 16:33:21.663784 140450407880576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/117M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 15.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 855013 tokens\n",
            "Training...\n",
            "[10 | 28.26] loss=3.07 avg=3.07\n",
            "[20 | 50.50] loss=3.02 avg=3.04\n",
            "[30 | 73.22] loss=2.92 avg=3.00\n",
            "[40 | 96.38] loss=2.81 avg=2.95\n",
            "[50 | 119.99] loss=2.56 avg=2.87\n",
            "[60 | 143.29] loss=2.78 avg=2.86\n",
            "[70 | 166.43] loss=2.44 avg=2.80\n",
            "[80 | 189.68] loss=2.59 avg=2.77\n",
            "[90 | 213.02] loss=2.43 avg=2.73\n",
            "[100 | 236.32] loss=2.64 avg=2.72\n",
            "[110 | 259.59] loss=1.86 avg=2.64\n",
            "[120 | 282.85] loss=2.13 avg=2.59\n",
            "[130 | 306.10] loss=2.18 avg=2.56\n",
            "[140 | 329.37] loss=2.51 avg=2.56\n",
            "[150 | 352.63] loss=1.60 avg=2.49\n",
            "[160 | 375.86] loss=2.27 avg=2.47\n",
            "[170 | 399.11] loss=2.09 avg=2.45\n",
            "[180 | 422.37] loss=2.31 avg=2.44\n",
            "[190 | 445.65] loss=2.39 avg=2.44\n",
            "[200 | 468.93] loss=2.59 avg=2.45\n",
            "======== SAMPLE 1 ========\n",
            " \"of all the great moments of the day: #60daysofudacity\" ] }, { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"finally the day comes:   \". <|endoftext|>\n",
            "<|startoftext|> thanks @bhadreshpsavani for being nice about your updates and keeping me on track . it's nice that we don’t have to wait to post updates.  <|endoftext|>\n",
            "<|startoftext|> day 1: 1. i made a small update to a github repository   so i can update it and check if the repo exists   2. i'm working on the project of pytorch.   3. i'm doing some basic object detection techniques so as to help me in the project. <|endoftext|>\n",
            "<|startoftext|> @bhadreshpsavani yes, it was. i tried to use google tf2, but it was too slow and didn’t work well for me. i think we will have to go back to google tf. <|endoftext|>\n",
            "<|startoftext|> day 2 : finally did  lesson 2/2 - federated learning! i’m not a huge fan of this one as i thought i would fail there. <|endoftext|>\n",
            "<|startoftext|> day 2 today: finished lesson 3. started working on kaggle datasets  and started working on the quiz on june 1st and completed all lesson 1 on kaggle 1. 0. 1. worked on my work to understand the concept of training with tensors and the related concepts of data exploration from python . 2. learnt about the difference between the training and evaluation of training data. <|endoftext|>\n",
            "<|startoftext|> thanks @fridarode00, it'll surely not go without saying, @sauravkumarsct@egreblova you are all kind of awesome guys. keep it up!! <|endoftext|>\n",
            "<|startoftext|> day2: 1. continued with lesson 3 lesson 3 on machine learning <|endoftext|>\n",
            "<|startoftext|> hi everyone, @aniketmitra1997 let's do it and have a nice day! <|endoftext|>\n",
            "<|startoftext|> *day 1* #60daysofudacity - watched andrew ng's video on secure federated learning -   - watched andrew ng's intro to deeplearning.ai -   - tried to understand how a database is structured and i am quite impressed with the explanation and the ability of @krishna.chari and @george.gogoll. i encourage @akathiangadi93 and @dubeankit07  to start with a good ol' streak! <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity day 1 progress on lesson 1 and lesson 2 completed i'd like to encourage @aaron.math and @kornelievelee :penguin_dance:  to continue with #60daysofudacity - with the new and bigger challenges! i'd like to encourage @fridarode00 and @sauravkumarsct to continue!!@abhigupta2957@akidzhafs@manishajhunjhunwala7 :penguin_dance: <|endoftext|>\n",
            "<|startoftext|> day 1 : today i completed lesson 3 in udemy.  - i'll be reviewing it more in the next project. <|endoftext|>\n",
            "<|startoftext|> day 2 of #60daysofudacity 1. completed lesson 4 2. started lesson 5 3. implemented pate analysis <|endoftext|>\n",
            "<|startoftext|> day 2 of #60daysofudacity 1. read about gans by cynthia dwork to see how they work and implement gans. 2. started lesson 5  (learn about *differentially private mit activation functions*).  *reading time15 min read**#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> *day 2 :*  • completed one day of lesson 2* • read another great study buddy article by @ltruncel • worked on another project. • implemented differential gan (decoction gradient descent)  as a cnn:learn more about gan (gradient descent optimizer) in this blogpost :   • read a study buddy article that discussed differential gans and implemented it in a cnn :   • also i’\n",
            "\n",
            "[210 | 504.26] loss=2.10 avg=2.43\n",
            "[220 | 527.56] loss=2.38 avg=2.42\n",
            "[230 | 550.85] loss=2.02 avg=2.41\n",
            "[240 | 574.15] loss=2.04 avg=2.39\n",
            "[250 | 597.45] loss=2.31 avg=2.38\n",
            "[260 | 620.77] loss=2.07 avg=2.37\n",
            "[270 | 644.10] loss=1.79 avg=2.35\n",
            "[280 | 667.43] loss=2.23 avg=2.34\n",
            "[290 | 690.73] loss=1.88 avg=2.32\n",
            "[300 | 714.02] loss=2.44 avg=2.33\n",
            "[310 | 737.30] loss=1.87 avg=2.31\n",
            "[320 | 760.59] loss=1.95 avg=2.30\n",
            "[330 | 783.90] loss=1.93 avg=2.28\n",
            "[340 | 807.23] loss=2.24 avg=2.28\n",
            "[350 | 830.54] loss=2.07 avg=2.28\n",
            "[360 | 853.88] loss=1.88 avg=2.26\n",
            "[370 | 877.22] loss=1.84 avg=2.25\n",
            "[380 | 900.54] loss=1.65 avg=2.23\n",
            "[390 | 923.84] loss=2.69 avg=2.24\n",
            "[400 | 947.11] loss=1.92 avg=2.23\n",
            "======== SAMPLE 1 ========\n",
            "                                                                                  ,                                                                                                                                                                                                                                                                                                                   { \"image_type\": \"mix\" }                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "\n",
            "[410 | 981.18] loss=1.86 avg=2.22\n",
            "[420 | 1004.42] loss=2.38 avg=2.23\n",
            "[430 | 1027.68] loss=1.96 avg=2.22\n",
            "[440 | 1050.95] loss=1.90 avg=2.21\n",
            "[450 | 1074.23] loss=1.74 avg=2.20\n",
            "[460 | 1097.51] loss=1.87 avg=2.19\n",
            "[470 | 1120.82] loss=1.86 avg=2.18\n",
            "[480 | 1144.10] loss=1.58 avg=2.17\n",
            "[490 | 1167.42] loss=1.73 avg=2.15\n",
            "[500 | 1190.74] loss=2.19 avg=2.16\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1217.56] loss=1.59 avg=2.14\n",
            "[520 | 1240.93] loss=1.81 avg=2.13\n",
            "[530 | 1264.23] loss=1.61 avg=2.12\n",
            "[540 | 1287.48] loss=1.71 avg=2.11\n",
            "[550 | 1310.74] loss=1.25 avg=2.09\n",
            "[560 | 1334.02] loss=1.68 avg=2.08\n",
            "[570 | 1357.35] loss=1.01 avg=2.06\n",
            "[580 | 1380.68] loss=2.13 avg=2.06\n",
            "[590 | 1404.02] loss=1.32 avg=2.04\n",
            "[600 | 1427.35] loss=1.57 avg=2.03\n",
            "======== SAMPLE 1 ========\n",
            "|text|>\n",
            "<|startoftext|> day 15: i went to a conference where we covered deep learning. i learned the basics of the pytorch and had a discussion with some people from udacity about it. (thank you @sarahhelena.barmer for shout-out @terwey2012 to watch the video) <|endoftext|>\n",
            "<|startoftext|> day 12 1) finished lesson 5 2) reading paper on softmax function :   3) read medium article - pd problem statement with minitropython <|endoftext|>\n",
            "<|startoftext|> day 15 - finish the course at phase 15 - continue the mini-project - write the next 5 days notebook day 16 - create a github project to track on github <|endoftext|>\n",
            "<|startoftext|> you will find the repo you requested here        on kaggle.com    | 1 | 1 | submit your pull request for the #sg_futuredevs     | | 2 | *start collecting syft data* | | | | *collecting materials on nlp* | | 1 | read *differential privacy for deep learning* | | | 1 | read *pate analysis* by cynthia dwork *federated learning**[intro to deep learning with pytorch] *learn and apply differential privacy while learning and using differential privacy in pytorch* | |  6 | i found this article interesting   i really enjoy the concept of learning and applying both when studying for the exam. i find it quite confusing at first and i cannot continue to solve simple things everyday but after a while it becomes easier and better. i would like to see the concept of the differentially private deep learning and its usefulness in deep learning. | |  #udacityfacebookscholars #udacity #sais <|endoftext|>\n",
            "<|startoftext|> day 15: 1. got selected to this competition 2. completed this section in my book:   progress of secure and private ai challenge:   -> learned how to: use a cnn or set of cnn as execution environment for our pytorch gui (introducing the pytorch gui). -> started lesson 5: introducing differential privacy. -> watched the last video: intro to local and global differential privacy. > read paper on deep learning:   from   to   <|endoftext|>\n",
            "<|startoftext|> day 14 [july 11th]: 1. studied lesson 4 deep learning with pytorch 2. studied lesson 3 :panda_dance: 3. read on neural networks in pytorch using machine learning algorithms. 4. updated my notes for the course on #udacity channel <|endoftext|>\n",
            "<|startoftext|> day 14 #14july# lesson 5 project intro federated learning working on project of part 4 - part 5 of federated learning #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> *day 15* - #60daysofudacity worked on lesson 7. - finished reading chapter 2, external the self-initiated self-learning scenario . - continue reading in *the algorithmic foundations of differential privacy* by cynthia dwork and aaron brown :party_mdm: - i encourage @davidblanco103 and @hantwin to join to take this challenge#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 15: #60daysofudacity • complete  lesson 9.3: securing federated learning • complete  lesson 9.4: federated learning #60daysofudacity updated8 hours ago <|endoftext|>\n",
            "<|startoftext|> *day 15:* i. continued on learning linear regression from the coursera coursera <|endoftext|>\n",
            "<|startoftext|> day 15/60 - #60daysofudacity i am sick after working the last few days on a computer project for self improvement. i would like to encourage @lakshmiangadi and @garg4.ag to follow in these steps. i am still trying to plan my next steps. <|endoftext|>\n",
            "<|startoftext|> day 14: watched some more videos of the videos of the study buddy club! 1. finished lesson 7. 3. revised lesson 5. i encourage @sauravkumarsct@sauravkumarsct@vova *not for beginners*#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day  11 completed #\n",
            "\n",
            "[610 | 1461.35] loss=1.81 avg=2.03\n",
            "[620 | 1484.62] loss=1.48 avg=2.01\n",
            "[630 | 1507.92] loss=1.46 avg=2.00\n",
            "[640 | 1531.27] loss=1.39 avg=1.99\n",
            "[650 | 1554.62] loss=1.60 avg=1.98\n",
            "[660 | 1577.94] loss=1.61 avg=1.97\n",
            "[670 | 1601.26] loss=1.47 avg=1.96\n",
            "[680 | 1624.59] loss=1.91 avg=1.96\n",
            "[690 | 1647.91] loss=1.57 avg=1.95\n",
            "[700 | 1671.25] loss=1.82 avg=1.95\n",
            "[710 | 1694.56] loss=1.73 avg=1.95\n",
            "[720 | 1717.88] loss=2.28 avg=1.95\n",
            "[730 | 1741.19] loss=1.96 avg=1.95\n",
            "[740 | 1764.52] loss=1.40 avg=1.94\n",
            "[750 | 1787.86] loss=1.51 avg=1.94\n",
            "[760 | 1811.21] loss=1.71 avg=1.93\n",
            "[770 | 1834.56] loss=2.40 avg=1.94\n",
            "[780 | 1857.89] loss=1.93 avg=1.94\n",
            "[790 | 1881.23] loss=1.32 avg=1.93\n",
            "[800 | 1904.55] loss=1.68 avg=1.92\n",
            "======== SAMPLE 1 ========\n",
            " best #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> #day 15 of #60daysofudacity 1. revised lesson 5 2. read a research paper on additive secret sharing 3. attended a meeting regarding our paper club <|endoftext|>\n",
            "<|startoftext|> day 15: :panda_dance: 1. i completed the week 2 of calculus - derivatives. 2. i revised my notes from the course - pate analysis and the formula of differentially private dl systems. 3. studied about inference, validation and dropout. 4. tried to learn and apply the fundamentals of data cleaning through the lessons 7.2: introducing local and global differential privacy#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 15 : 1. still on the course in federated learning about how to implement the project in pytorch 2.  worked on the project and code snippets for lesson 4: differential privacy for deep learning. 3. read a research paper on facebook's new mlp feature prediction project :  i encourage @shivamraisharma to share the link :   i encourage @nabhu123@adrielmvieira@khandelwalcibaca@chocolate.coffee to keep going with #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 2  of #60daysofudacity : started the first mini-project which i watched on youtube -   :torch_heart_small: how to create a mini-project like this  a big shout out to: @ewotawa@sfmajors373@lexie@chocolate.coffee for making my daily list  a big shoutout to @khandelwalcibaca and @dushyantpathak6789 who made the pledge today ! <|endoftext|>\n",
            "<|startoftext|> day 14: working on gcp interview with machine learning scientist @arkachkrbrty. <|endoftext|>\n",
            "<|startoftext|> day 13/60 - wednesday, 1 july 2019 • the session on federated learning revsive with a visual exploration of the topic and analysis of all the ideas in ai articles. • the meeting with  @arkachkrbrty and  @ahmedthabit99. • working on the project on pneumothorax segmentation. • looking forward for the meet-up and participating. <|endoftext|>\n",
            "<|startoftext|> day 14: 1. finished my second part of the fast.ai challenge 2. continued lesson 5 3. started reading related articles 4. read articles about data analysis, machine learning and deep learning. i would like to encourage @sukhwal.niki@anjumercian85@bharati.frnds@urvi151197@arkachkrbrty@calincan2000@mjmolinacontreras@f2014360. #udacityfacebookscholar #udacity #facebookai #facebookcholaropenmined.org is an open-source community focused on researching, developing, and elevating tools for secure, privacy-preserving, value-aligned artificial intelligence. <|endoftext|>\n",
            "<|startoftext|> #day15 lesson 8 1. continue lesson 5 2. join openmined study group (michael r. from colombia) <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity  *day 14:* • read chapter 4 of *grokking deep learning* and finished it. • finish my first lecture of *grokking deep learning* for *pytorch* course. i won't spoil it for anyone, but if someone is interested, please let me know. • started reading *\"the formal definition of differential privacy\"* by cynthia dwork. i can't wait until tomorrow to read it.  <|endoftext|>\n",
            "<|startoftext|> day 14: - went through videos from federated learning. hope next three days won't be difficult  - finally took the first few steps to build my first multi-label data set. i learned the solution in less than 30 seconds. what i also discovered - i cannot predict every change. i think it will be very hard to learn. i encourage @pratyaksha6696@ziad.esam.ezat@sukhwal.niki@balyseviene to join us. <|endoftext|>\n",
            "<|startoftext|> day 11 : i updated my #60daysofudacity project (on github) with some updates.\n",
            "\n",
            "[810 | 1938.65] loss=1.32 avg=1.91\n",
            "[820 | 1961.92] loss=1.51 avg=1.91\n",
            "[830 | 1985.19] loss=1.32 avg=1.90\n",
            "[840 | 2008.46] loss=1.19 avg=1.88\n",
            "[850 | 2031.73] loss=1.18 avg=1.87\n",
            "[860 | 2054.99] loss=1.53 avg=1.87\n",
            "[870 | 2078.27] loss=1.29 avg=1.86\n",
            "[880 | 2101.59] loss=1.33 avg=1.85\n",
            "[890 | 2124.87] loss=1.13 avg=1.83\n",
            "[900 | 2148.18] loss=1.23 avg=1.82\n",
            "[910 | 2171.48] loss=1.26 avg=1.81\n",
            "[920 | 2194.81] loss=1.26 avg=1.81\n",
            "[930 | 2218.11] loss=1.31 avg=1.80\n",
            "[940 | 2241.41] loss=1.35 avg=1.79\n",
            "[950 | 2264.71] loss=1.36 avg=1.78\n",
            "[960 | 2288.00] loss=1.21 avg=1.77\n",
            "[970 | 2311.27] loss=1.09 avg=1.76\n",
            "[980 | 2334.55] loss=1.28 avg=1.76\n",
            "[990 | 2357.84] loss=1.29 avg=1.75\n",
            "[1000 | 2381.13] loss=0.96 avg=1.74\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0801 17:13:11.830547 140450407880576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "Copy the trained model to the Google Drive. The checkpoint folder is copied as a `.rar` compressed file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Now that we have trained and/or loaded our finetuned model, its time to generate text! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "c3b63fb2-9b9b-4128-e896-6e4b071a04ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "prefix: force the text to start with a given character sequence and generate \n",
        "  text from there \n",
        "nsamples: generate multiple texts at a time\n",
        "batch_size: generate multiple samples in parallel, giving a massive speedup \n",
        "  (in Colaboratory, set a maximum of 20 for batch_size)\n",
        "length: Number of tokens to generate (default 1023, the maximum)\n",
        "temperature: The higher the temperature, the crazier the text \n",
        "  (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "top_k: Limits the generated guesses to the top k guesses \n",
        "  (default 0 which disables the behavior; if the generated output is \n",
        "  super crazy, you may want to set top_k=40)\n",
        "top_p: Nucleus sampling: limits the generated guesses to a \n",
        "  cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "truncate: Truncates the input text until a given sequence, excluding that\n",
        "  sequence (e.g. if truncate='<|endoftext|>', the returned text will include \n",
        "  everything before the first <|endoftext|>). It may be useful to combine this\n",
        "  with a smaller length if the input texts are short.\n",
        "include_prefix: If using truncate and include_prefix=False, the specified \n",
        "  prefix will not be included in the returned text.\n",
        "'''\n",
        "\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=512,\n",
        "              temperature=0.8,\n",
        "              prefix=\"Day: \",\n",
        "              nsamples=10,\n",
        "              batch_size=10\n",
        "             )\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Day:  done the project  <|endoftext|>\n",
            "<|startoftext|> @mjmolinacontreras you had better get ready for some problems!!! and congratulations!!  i was wondering where to learn this !! <|endoftext|>\n",
            "<|startoftext|> day 1: finished the videos of lesson 4. had to file a bug report with the firebase security team.   happy learning !! <|endoftext|>\n",
            "<|startoftext|> day 1: 1. took the pledge of #60daysofudacity 2. finished lesson 2.3 3. started lesson 2.4 and started to understand the concepts deeply. 4. i would like to invite @preriec and @eileen.hertwig to join #60daysofudacity <|endoftext|>\n",
            "<|startoftext|> *day:* 1. i took the pledge of #60daysofudacity. 2. completed 60 minute blitz of linear algebra in python 3. completed lesson 1 of intro to dl with pytorch youtube series 4. i would like to encourage @birozso, @casabiancadenny, @mariia.denysenko93, @arkachkrbrty <|endoftext|>\n",
            "<|startoftext|> *day 1:* 1. i took the pledge 2. completed lesson 3 3. i encourage @sarahhelena.barmer and @djnavin619 to take the pledge if they haven't already. 4. i would like to encourage @sankalpdayal5, @sarahhelena.barmer and @amansharma.pcmip@electric.blake@jenashubhangi20@bharati.frnds@sayalikutwal26@bharati.frnds@naimashaikh550@quratfatima581@ikhverksintme <|endoftext|>\n",
            "<|startoftext|> *day 1 :* 1. took the pledge of #60daysofudacity. 2. started lesson 8. 3. completed 2 assignments of the lesson 8. 4. i'd like to encourage @venkkratos, @rkshntn, @rkshntn.47, @rkshntn.55, @sarahhelena.barmer,\n",
            "====================\n",
            "Day:  #60daysofudacity 1. watched lesson 8 \"evaluating the privacy of a function\" 2. implemented project demo for #60daysofudacity in kaggle arev's and here i reproduce the code 3. tried to determine pca value for lak terms found on google encoding/decoding books \"deep learning with pytorch\" <|endoftext|>\n",
            "<|startoftext|> day 3: 1. completed videos of lesson 6 2. studied some pytorch concepts in introductory pytorch course by vaanishing ray leopold lectures on fastai and nlp <|endoftext|>\n",
            "<|startoftext|> day 3: ~today i spent around 4 hours working on some projects and also reading few resources from #resources   ~i joined a study group with @sol.vriksh@nabhu123@alegalindo-101@drcjudelhi@alegalindo-101@0107wy@vincivenv@d.pandey11585@alegalindo-101@d.pandey11585@d.pandey11585@alegalindo-101@d.pandey585@alegalindo-101@d.pandey585@alegalindo-101 <|endoftext|>\n",
            "<|startoftext|> day 3: 1- completed the first 3 videos of lesson 6 2- started with the remaining concepts in lesson 7 3- started learning about local and global differential privacy ---------------------------- cheers for @susyjame@amansharma.pcmip@singmin_92@zohra.mes96@tsmedia.rj <|endoftext|>\n",
            "<|startoftext|> day 2: #60daysofudacity 1. completed lesson 2.6 2. completed lesson 2.7 3. read some articles on data analysis. 4. started learning computer vision from reading this tutorial.comic book to brush up on deep learning. 5. i would like to encourage @sankalpdayal5  & @sharmin.cse1.bu to participate in the challenge! <|endoftext|>\n",
            "<|startoftext|> day 3: 1. continuing with lesson 2 and also revised some concepts  2. finished solving all of the notes for lesson 2 till around 75\n",
            "====================\n",
            "Day:  [m4s: ok, let's revisit it. let's say i'm watching a certain movie and then i'm thinking of trying to build a small personal computer to recognise this particular voice.]] :thinking_face: [how do we say \"using the computer vision of text to recognise\" images? because this is a text synthesiser, it's almost like a text recogniser.] [it can also be thought of as a medical device with a computer vision module, and it could be built using text, but this kind of device would not have access to deep learning.] [to recap: the idea is that if you could build a robotic device that could recognise text images, it would be much easier, and the same would be true of this ai-based text synthesiser. this will be a purely ai-based device, and would not rely on ai. we could also opt for a fully-automated construction – where parts are already in use and the parts are no longer needed – and then convert the electrical current generated by the robot into electrical signals and signal (e.g. using a torch to make a lightbulb.)[1] :thinking_face: [edited] :thinking_face: [1] will be written at the end of the first chapter of chapter 2, \"the algorithmic foundationsoftext synthesisers\". i'm using a very basic text synthesiser, which is made from some randomised number generator (random-1):   [1] i want to encourage @eileen.hertwig, @egreblova, @elena.kutanov, @bharati.frnds, @sachungo1, @btliu3] <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity day 4 learned the formal definition of differential privacy <|endoftext|>\n",
            "<|startoftext|> day 5 1. continued with the lesson 3 2. continued reading the algorithmic foundations of differential privacy <|endoftext|>\n",
            "<|startoftext|> day 4, 5, 6 finished lesson 3, learned about the pate analysis. @btliu3 i have added the lesson to my github repo. <|endoftext|>\n",
            "<|startoftext|> day 1: i took the pledge. finally i can do it from the comfort that i am in udacity! <|\n",
            "====================\n",
            "Day:  splitted models with pre trained models  -   -   implemented a model with a fully connected, supervised and unconnected neighbor dataset, generating gradients for each of the 20 labeled classes. -    <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity day 6: 1. continue the lesson 5 and complete the concept of the formal definition of differential privacy. 2. started learning 2,3,4 of intro to deep learning with pytorch course on udacity <|endoftext|>\n",
            "<|startoftext|> *day 6/60* 1. finished off lesson 8, working on the last exercise i had to do to help with the first lesson (introduction to pytorch) 2. went through some sources on how to optimize, learn better using open source, and i hope to contribute more in the future articles (i have learned more since starting this challenge, learned a lot from different channels) ------------------------------------- happy learning <|endoftext|>\n",
            "<|startoftext|> day 3: spent a few hours reviewing, understanding some concepts of differential privacy and also planned for a virtual meet-up with my study group@sanskriti.bajaj98@bharati.frnds@ivolinengong@sanskriti.bajaj98@anjumercian85@learn556@krisastern <|endoftext|>\n",
            "<|startoftext|> day 5: #60daysofudacity completed : completed all 4 lectures of lesson 1 till lesson 4 successfully <|endoftext|>\n",
            "<|startoftext|> *day 6/60!* today i had a meeting with some people from the udacity group. we talked about the course progress and we each decided to drink a few hours to get fit again. i joined 2 study groups        *#60daysofudacity* #sg_work_using_gcp *#60daysofudacity* data scientists : join 2 study groups     *#startuptake* : 1. start taking *#*takeout* course on udacity. :in*help: 2. read about:   start working on *blindness detection* with *blindness detection for blindness* implementation. :jobs:: 3. join 2 study groups   <|endof\n",
            "====================\n",
            "Day:  answered questions on #tech_help!  *day 3/60* 1. hosted the first virtual meetup of #sg_cognitive-learners by @sanskriti.bajaj98 2. completed lesson 7 3. preparing for virtual meet up 4. reading and coding   i would like to encourage @sarahhelena.barmer@jenashubhangi20@mushrifahhasan@minassouane@mhmohona to take up the challenge today <|endoftext|>\n",
            "<|startoftext|> day 3: #60daysofudacity 1. completed lesson 3 : introducing differential privacy 2. : evaluating the privacy of a function : <|endoftext|>\n",
            "<|startoftext|> day 2: #60daysofudacity 1. worked on lesson 2 and joined the reading_club. 2. revised lesson 2 and other deep learning topics <|endoftext|>\n",
            "<|startoftext|> day 2: #60daysofudacity 1. completed lesson 1 and started working on the project 2. read some articles on backpropagationlanguagejupyter notebook <|endoftext|>\n",
            "<|startoftext|> day 1: #60daysofudacity 1. completed lesson 1 and started working on the project 2. started working on the lesson 2 projects. <|endoftext|>\n",
            "<|startoftext|> *day 3* of #60daysofudacity: -finished lesson 7 -working on the final project in lesson 8 -started mini projects i will be tackling during this week: -intro to pytorch course from udacity on lesson 6: -build a simple neural network using keras and later extend it with nn from 1 to 6 with the help of pytorch essentials i spent 30 minutes today i think, to speed up the learning pace of the course and make it more digestible for all. i will be sharing the link of my progress during the week <|endoftext|>\n",
            "<|startoftext|> day 2: #60daysofudacity 1. finished lesson 6 2. started working on a project 3. read few hyperparameters of lesson 5. <|endoftext|>\n",
            "<|startoftext|> day 2 of #60daysofudacity: started lesson 2 : introduction to neural\n",
            "====================\n",
            "Day:    @eagle_phoenix2000 thank you too for your encouragement always   thank you for your tag, you are always an inspiration  <|endoftext|>\n",
            "<|startoftext|> thank you - happy 4th - day, i learned how to train a model on production servers and posted the problem in my repo! i encourage @egreblova, @eagle_phoenix2000, @preriec, @workwithpurpose, @ragnagus, @aisha.jv70, @akarhade5, @vimaayi <|endoftext|>\n",
            "<|startoftext|> day 9: 1. completed lesson 6 2. coded and trained a model for training neural network models in python 3. wrote a comprehensive excercise in the book chapter 4 in detail <|endoftext|>\n",
            "<|startoftext|> day 12: 1. completed lesson 5 2. worked on interview processing in a variety of kaggle competitions @xanderfomenko@munniomer and @raghavansandhya#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 12: worked on a kaggle competition   :60daysofudacitybadge:   :torch_heart_big: read an interesting article about hyper parameter tuning :torch_heart_big: played around with the code for an hour#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 9 of #60daysofudacity: 1. implemented models and algorithms using them and constructed the models using them and got the attributes:   2. completed the differential privacy section from the course 2. read about    labs 3. read about   4. experimented with differentially private labels 5. i'd like to encourage @michael092001@quratfatima581 to continue participating in this challenge! <|endoftext|>\n",
            "<|startoftext|> day 13: #60daysofudacity 1. completed garbage collection with a big fat carrot:   2.  read some gans on the web:   3.  started lesson 4: evaluating the privacy of a function <|endoftext|>\n",
            "<|startoftext|> day 12: started working on the secure federated learning project. checked\n",
            "====================\n",
            "Day:  #60daysofudacity  :penguin_dance: :penguin_dance:  :penguin_dance: *day4* - completed lesson 3 - toy federated learning with trusted aggregator (predicting prices using adversarial autoencoder) - federated learning using the *federated learning* framework - using federated learning to train our model - generating randomized couplets for trading pair data - read *the foundations of differential privacy* by cynthia dwork - start reading *\"the algorithmic foundations of differential privacy\"* (courtesy of my study group mates: @terwey2012, @raghavansandhya26 and @kumarprabhu244) - #sg_project-t-shirt has just posted the first survey! we posted the questions regarding t-shirt, t-shirt-federated-learning and t-shirt-project-t-shirt to #sg_project-t-shirt! some people have already taken the pledge, so take the pledge now and hope to be in the top 32% by today! :penguin_dance: *stay udacious*@ikhushpatel@andreiliphd@katrine.chow@mudeledimeji <|endoftext|>\n",
            "<|startoftext|> day 4: completed in portions 1-15. watched video on *deep learning with pytorch* from \"intro to deep learning with pytorch\" course. <|endoftext|>\n",
            "<|startoftext|> day 4:  #60daysofudacity  revised lesson 3  read about *neural networks* in research paper play with video (rectifying incorrect images) of \"intro to deep learning with pytorch\" course. <|endoftext|>\n",
            "<|startoftext|> #60daysofudacity day 3: 1. i updated my #60daysofudacity project in github:   2. i finished watching the first two videos of lesson 2, until i am ready to code the third. i am not working on the final project, but i am looking forward to learn how it is done and practice with it.#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 4: done my day 2 with a client's project (delivering positive emotions) on\n",
            "====================\n",
            "Day:  #60daysofudacity  #day5 : 1) learn the formal definition of differential privacy (lesson 3) 2) review lesson 2 \"convolutional neural networks\" in introduction to deep learning in purple and understand the basics in tutorial <|endoftext|>\n",
            "<|startoftext|> day 6: 1- doing a recap of lesson 6 2- checking out some pytorch projects for beginners 3- continued trading in the kaggle sql summer camp: prepping to conduct another session and compete on the ama session today <|endoftext|>\n",
            "<|startoftext|> day 8: completed all lessons but one, cleaned up the workspace and held hands with the #sg_wonder_vision fan club team <|endoftext|>\n",
            "<|startoftext|> day 6/60 watched some youtube videos and read some articles based on lesson 3(introducing local and global differential privacy)#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 8: did u finished lesson 8 yesterday, then got confused by some classes in differential privacy <|endoftext|>\n",
            "<|startoftext|> day 4 of #60daysofudacity: 1. completed lesson 8 2. read paper on additive secret sharing: additive secret sharing in particular is an important subject with many unanswered questions. how should we implement this in the real world of nlp, deep learning, ml, dl, etc... 3. started reading about gan's (global differential privacy) and i am currently reading about functions that allow you to preserve a database. i am currently in the lna, but i will share my documentation soon on the following page. i'd suggest reading these & more as soon as they appear!#60daysofudacity #udacityfacebookscholar <|endoftext|>\n",
            "<|startoftext|> day 7: started working in convolution networks for lossy classification problems. tagging @dragomirescu200, @calincan2000, @cioloboc.florin, @mateusz_zatylny, @nau71930. <|endoftext|>\n",
            "<|startoftext|> *day 8*: 1. completed part 7 and 8 of lesson 6 2. continued learning tutorial in #l8_encrypted_dl <|endoftext|>\n",
            "====================\n",
            "Day:    #60daysofudacity  :penguin_dance: :bat_parrot: :bat_parrot:  1. learn pytorch and pysyft + pysyft in fast.ai course  :penguin_dance: :bat_parrot: :bat_parrot:  2. learn pytorch tutorials in vue.js book  :penguin_dance: :bat_parrot: :bat_parrot:  i would like to encourage @egreblova, @eileen.hertwig, @sibrahim1396, @souvikb1812, @labknr98, @sachungo1, @elimao_dance:  i would also like to tag and encourage @souvikb1812@egreblova@elena.kutanov@lexie@minassouane@seeratpal91@erinmoo@temitopeo49@agbugbaijeoma@akashsinsugab@akamilkhan@ikhushpatel@adventuroussrv@ashishiva3@avinashsobi to join me up! let's do this! :the_horns: :the_horns: :the_horns:  <|endoftext|>\n",
            "<|startoftext|> day 2: 1. lesson 2 completed. 2. completed reading dp book by goodfellow. 3. learned how to use google colab. 4. i'm tagging @rishisridhar96@aysha.kamal7@sharmaanix@poojavinod100@lexie@jk5128@ravikantsingh2308@jk5128@jagdollis <|endoftext|>\n",
            "<|startoftext|> day 2: 1. completed till lesson 3 2. completed lesson 4 3. i am tagging @ash3ax@anshutrivedik@amansharma.pcmip@161210032@terwey2012@terwey2012@vimaayi <|endoftext|>\n",
            "<|startoftext|> *day-2:*  studied the following papers: (1) log-translator (2) implementing a differential privacy mechanism@arkachkrbrty,\n",
            "====================\n",
            "Day:  had a quick virtual meetup with my study buddies and my study group. read about kaggle and java self-driving car project. i encourage @electric.blake, @drcjudelhi, @clintonasoh, @cclinc2005 to join the #60daysofudacity challenge! time to go :+1: <|endoftext|>\n",
            "<|startoftext|> day 6: 1. completed lesson 4, and completed the next 2 projects of the chapter 2. started watching the latest lesson (introducing differential privacy)#60daysofudacity <|endoftext|>\n",
            "<|startoftext|> day 5: 1. continued lesson 3 2. started writing a paper on *differentially private federated learning: a client level perspective* 3. started my journey of   encouraging @sarahhelena.barmer@learn556@shashi.gharti@krishna.chari to complete this challenge and share it with us  <|endoftext|>\n",
            "<|startoftext|> day 6: 1. completed all chapters in lesson 5, and started build the lesson 6 differential privacy for deep learning. 2. completed all chapters in lesson 6, and completed the building the model and practicing the concepts in real world. <|endoftext|>\n",
            "<|startoftext|> day 3: 1. completed all notebooks of lesson 2 2. revised lesson 3 3. read the book deep learning by ian goodfellow 4. read through various medium discussion boards such as #channel_midsize_sample_kaggle/ and #sg_applied_dl 5. i encourage @manishajhunjhunwala7@arkachkrbrty@debankurrocks@shudiptotrafder@adventuroussrv@aniketthomas27@nirupama.it@guptays - it means a lot to me! :penguin_dance: :clapping: :bettertogether: - i encourage everyone to keep up the streak and post a #60daysofudacity daily posting!   <|endoftext|>\n",
            "<|startoftext|> *day 5:* #60daysofudacity completed notebooks of securing federated learning continued working on tutorial projectsstarted to learn pytorch from official pytorch documentation swatched some people from #sg_\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Troubleshoot\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail or out-of-memory/OOM), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}