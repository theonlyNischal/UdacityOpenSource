# Training Sixty
Sixty AI was trained on a text corpus of extracted messages from a Udacity Slark Archive Channel called _60 Days of Udacity_ during the [Facebook's Secure and Private AI Scholarship](https://www.udacity.com/facebook-AI-scholarship "Facebook's Secure and Private AI Scholarship"). _As part of the #60DaysOfUdacity challenge, participants had to update their progress in the challenge course daily in the slack channel for 60 days at a stretch._ Hence, the text generated by Sixty closely resembles the updates people were posting in the channel. 

<br/>Sixty harnesses the power of OpenAI's GPT-2 model to generate texts. Specifically, their smallest version called _**117M**_. However, raw GPT-2 117M was not used for the task as the original model lacks the '_Finetuning_' capability. Instead, a fork of GPT-2 by _Max Woolf_ called `gpt-2-simple` was used. 

<br/>`gpt-2-simple` library provides a simple command line interface to finetune a GPT-2 model while also integrating seamlessly with Google Drive. Since the models were trained in Google Colab, integration with Google Drive proved to be super useful. For more about `gpt-2-simple`, you can visit [it's GitHub repository](https://github.com/minimaxir/gpt-2-simple).

<br/> The notebook in this folder lays out a straightforward template to train the GPT-2 on any text corpus. You simply need to _plug-in_ the text corpus (can be a csv or text file), and the `gpt-2-simple` library takes care of everything else. Read the markdowns and the comments in the notebook to know more. 

<br/>_**NOTE:**_ The notebook in this folder is a Google Colab notebook. It is recommended that you run the notebook in Colab if your system doesn't have a GPU.